<#

# The Flyway Teamworks Scriptblocks.

These are a collection of script blocks that are designed to run with Flyway either in the community edition or with Teams. They allow you to maintain backups or scripts for each version and make a Flyway a full participant in any source control system in use, even to the point of providing an  object-level source to allow the evolution of individual tables to be tracked.

Some of these will work on the major relational database systems, whereas some are for SQL Server only 

A simple database that is really nothing more than a grown-up spreadsheet will  not need any of this fancy stuff. It becomes increasingly important as the database grows into a team effort. The facilities cover automatic code reviews, source control, and change tracking. It provides build scripts that are, I believe, essential to successful branching and merging, and for databases such as MySQL that do not roll-back DDL changes in failed transactions. Some of these utilities are there primarily to show how to run external programs, or to save the results of a SQL Call as a JSON file or other result. I have  examples of SQL Code executed from a variable containing a query,  and from  a file. 

They share a common data object (hashtable) as a parameter. This is automatically  generated by the preliminary.ps1 that also obligingly retrieves your password from your local secure store. This hashtable has all the parameters set by the config  files and environment variables. It parses the URL provided to Flyway to retrieve the name of the database server, port and the database. Each scriptblock checks to see whether you've provided what is needed... the parameter is actually a hashTable passed by reference. 
Few of these parameters  are needed for any one task, and some such as version and password are filled in as the hashtable is accessed.

These Scriptblocks can be batched up into an array and processed one by one. Some of them are utility chores such as extracting passwords from a relatively safe place, or fetching prepared parameters. There
is one that merely formats the data. They are able to write errors or status to the data object. The hashtable is passed by reference. One has to be cautious with this because alterations to the hashtable can result in powerShell holding a copy instead. 

As well as being run in a series, they can be used individually. If one of them needs maintenance, it is very easy to pull it apart and run it interactively.

The reason for using this design was to make it easy to choose what gets run and in what order. 

**$CheckCodeInDatabase** *(SQL Server only)*
This scriptblock checks the code in the database for any issues, using SQL Code Guard to do all the work. This runs SQL Codeguard  and saves the report in a subdirectory the version directory of your project artefacts. It also reports back in the **$param1** Hashtable. It checks the current database, not the scripts

**$CheckCodeInMigrationFiles** *(SQL Server only)*
This scriptblock checks the code in the migration files for any issues, using SQL Code Guard to do all the work. This runs SQL Codeguard and saves the report in a subdirectory the version directory of your 
project artefacts. It also reports back in the **$param1** Hashtable. It checks the scripts not the current database.

**$CreateScriptFoldersIfNecessary**: *(SQL Server, SQLite, MySQL, MariaDB, PostgreSQL)*
this task checks to see if a Source folder already exists for this version of the database and, if not, it will create one and fill it with subdirectories for each type of object. A tables folder will, for example, have a file for every table each containing a build script to create that object. When this exists, it allows SQL Compare
to do comparisons and check that a version has not drifted. It saves the Source folder as a subfolder for the supplied version, so it needs **$GetCurrentVersion** to have been run beforehand in the chain of tasks.

**$CreateBuildScriptIfNecessary**: *(PostgreSQL, MySQL, MariaDB,SQL Server, SQLite)*
produces a build script from the database, using SQL Compare. It saves the build script in the Scripts folder, as a subfolder for the supplied version, so it needs $GetCurrentVersion to have been run beforehand in the chain of tasks.

**$ExecuteTableSmellReport** *(SQL Server only)*
This scriptblock executes SQL that produces a report in XML or JSON from the database that alerts you to tables that may have issues

**$ExecuteTableDocumentationReport** *(SQL Server,MySQL, MariaDB, PosgreSQL)*
 This places in a report a json report of the documentation of every table and its columns. If you add or change tables, this can be subsequently used to update the **AfterMigrate** callback script
for the documentation. 

**$FetchAnyRequiredPasswords** *(all RDBMSs)*
This checks the hash table to see if there is a username without a password. If so, the task asks for it in a query window and stores it, encrypted, in the user area it. If the user already has the password stored for this username and database, it uses it.

**$ExecuteTableSmellReport** *(SQL Server only)*
This scriptblock executes SQL that produces a report in XML or JSON from the database

**$GetCurrentVersion** *(PostgreSQL, MySQL, MariaDB,SQL Server, SQLite)*
This contacts the database and determines its current version, and previous version by interrogating the flyway_schema_history data table in the database. If it is an empty database,or there is just no Flyway data, then it returns a version of 0.0.0.

**$GetCurrentServerVersion** *(PostgreSQL, MySQL, MariaDB,SQL Server, SQLite)*
This scriptblock gets the current version of the RDBMS on the server and is used mainly to check that the migration doesn't use any functionality that can't be supported on that server version. It updates
the $Param1.ServerVersion 

**$IsDatabaseIdenticalToSource:** *(SQL Server only)*
This uses SQL Compare to check that a version of a database is correct and hasn't been changed. To do this, the $CreateScriptFoldersIfNecessary task must have been run first. It compares the database to the associated source folder, for that version, and returns, in the hash table, the comparison equal to true if it was the same, or false if there has been drift, with a list of objects that have changed. If the comparison returns $null, then it means there has been an error. To access the right source folder for this database version, it needs $GetCurrentVersion to have been run beforehand in the chain of tasks

**$SaveDatabaseModelIfNecessary** *(PostgreSQL, MySQL, MariaDB,SQL Server, SQLite)*
This writes a JSON model of the database to a file that can be used subsequently to check for database version-drift or to create a narrative of changes for the flyway project between versions.

**$BulkCopyIn** *(SQL Server only)*
This script performs a bulk copy operation to get data into a database. It can only do this if the data is in a suitable directory. At the moment it assumes that you are using a DATA directory at the same level as the scripts directory. 
BCP must have been previously installed in the path Unlike many other tasks, you are unlikely to want to do this more than once for any database.If you did, you'd need to clear out the existing data first! It is intended
for static scripts AKA baseline migrations.

**$BulkCopyout** *(SQL Server only)*
This script performs a bulk copy operation to get data out of a database, and into a suitable directory. At the moment it assumes that you wish to use a DATA directory at the same level as the scripts directory. 
BCP must have been previously installed in the path.

**$CreateUndoScriptIfNecessary** *(SQL Server only)*
this creates a first-cut UNDO script for the metadata (not the data) which can be adjusted and modified quickly to produce an UNDO Script. It does this by using SQL Compare to generate a  idepotentic script comparing the database with the  contents of the previous version.

**$GeneratePUMLforGanttChart** *(All RDBMSs)*
This script creates a PUML file for a Gantt chart at the current version of the database. This can be read into any editor that takes PlantUML files to give a Gantt chart

**$CreatePossibleMigrationScript** *(SQL Server only)*
This creates a forward migration that scripts out all the changes made to the database since the current migration

**$SaveFlywaySchemaHistoryIfNecessary** *(all RDBMSs)*
This reads the flyway history table, and uses the information to annotate the directories containing the various reports and scripts for that version

**$CreateVersionNarrativeIfNecessary** *(PostgreSQL, MySQL, MariaDB, SQL Server, SQLite)*
This aims to tell you what has changed between each version of the database. 

**$WriteOutERDiagramCode** *(PostgreSQL, MySQL, MariaDB, SQL Server, SQLite)*
This creates a simple entity diagram for the current version. You only need two files to do this and you don't need to contact the database. The ER diagram has all objects that are either added, removed or changed colour-coded so you can see immediately what has changed. The idea of this is to be able to paste the resulting SVG file or other image file of the diagram, produced by PlantUMLc.exe.


## examples of usage

Tasks can be executed one at a time or stacked up and executed one after another. 

Here are several being done together 

```
`$PostMigrationTasks = @(`
	`$GetCurrentVersion, #checks the database and gets the current version number`
	`#it does this by reading the Flyway schema history table.` 
    `$GetCurrentServerVersion, #get the current version of the database server`
	`$CreateBuildScriptIfNecessary, #Create a build script for the database in a` 
	`#subdirectory for this version.`
	`$SaveDatabaseModelIfNecessary, #Build a JSON model of the database that we can`
	`#later use for comparing versions to create a chronicle of changes.`
	`$CreateVersionNarrativeIfNecessary,`
    `#save the information from the history table about when all the changes were made and by whom
â€‹    `$SaveFlywaySchemaHistoryIfNecessary`
`)`
`Process-FlywayTasks $param1 $PostMigrationTasks`
â€‹	`}`
`}`
```

here is one scriptblock being done

``` 
Process-FlywayTasks $param1 $GetCurrentServerVersion 
```

Some scriptblocks have extra parameters that allow them to be used more freely. 

here is an ERD diagram being done for a different version of the current project

``` 
Process-FlywayTasks $dbDetails $WriteOutERDiagramCode @('1.1.6')
```

In this case, there are other parameters that can be changed but they are ignored if set to NULL

```
Process-FlywayTasks $dbDetails $WriteOutERDiagramCode @(
		'1.1.7', #version - the flyway version of the database. Leave null if using framework
		  $null, #Title - the flyway project. Leave null if using framework
          $null, #FileLocations - where to store all files
		  $null, #MetadatachangeFile - Specify if not using the default location
		  $null, #modelFile - Specify if not using the default location
		  $null  #MyPUMLFile - The path to the PUML file
)
```

Here we change the title and the location of the files

```
Process-FlywayTasks @{
problems=@{};warnings=@{};feedback=@{};writeLocations=@{}
} $WriteOutERDiagramCode @(
		'1.1.7', #version - the flyway version of the database. Leave null if using framework
		'MyTitle', #Title - the flyway project. Leave null if using framework
        'MyFileLocation', #FileLocations - where to store all files
		$null, #MetadatachangeFile - Specify if not using the default location
		$null, #modelFile - Specify if not using the default location
		$null  #MyPUMLFile - The path to the PUML file
);Process-FlywayTasks $dbDetails $WriteOutERDiagramCode @('1.1.6')
```

You might not want all the project array because you're just generating diagrams from the two model files. Why not? So you just do the bare minimum hashtable

```
`Process-FlywayTasks @{`
`problems=@{};warnings=@{};feedback=@{};writeLocations=@{}`
`} $WriteOutERDiagramCode @(`
		`'1.1.7', #version - the flyway version of the database. Leave null if using framework`
		`'MyTitle', #Title - the flyway project. Leave null if using framework`
        `'MyFileLocation', #FileLocations - where to store all files`
		`$null, #MetadatachangeFile - Specify if not using the default location`
		`$null, #modelFile - Specify if not using the default location`
		`$null  #MyPUMLFile - The path to the PUML file`
`);`
```

#>


$GetdataFromSqlite = { <# a Scriptblock way of accessing SQLite via a CLI to get JSON-based  results 
without having to explicitly open a connection. it will take either SQL files or queries.  #>
	Param (
		$Theargs,
		#this is the same ubiquitous hashtable 
		$query,
		#a query. If a file, put the path in the $fileBasedQuery parameter
		$fileBasedQuery = $null) # $GetdataFromSqlite: (Don't delete this)
	$problems = @()
    $command=$null;
    $command = get-command sqlite -ErrorAction Ignore 
	if ($command -eq $null)
	  {  
         if ($sqliteAlias -ne $null)
        {Set-Alias sqlite $sqliteAlias}
    else
        {$problems += 'You must have provided a path to sqlite.exe in the ToolLocations.ps1 file in the resources folder'}
    }
	
	if ($TheArgs.Database -in @($null, '')) # do we have the necessary values
    { $problems += "Can't do this: no value for the database" }
	
	if ($problems.Count -eq 0)
	{
		$TempInputFile = "$($env:Temp)\TempInput$(Get-Random -Minimum 1 -Maximum 900).sql"
		if (!([string]::IsNullOrEmpty($FileBasedQuery))) #if we've been passed a file ....
		{ $TempInputFile = $FileBasedQuery }
		else
		{ [System.IO.File]::WriteAllLines($TempInputFile, $query); }
		
		$params = @(
			'.bail on',
			'.mode json',
			'.headers off',
			#".output $($TempOutputFile -replace '\\','/')",
			".read $($TempInputFile -replace '\\', '/')",
			'.quit')
        
		try
		{
			$result = sqlite "$($param1.database)" @Params
		}
		catch
		{ $problems += "SQL called to SQLite  failed because $($_)" }
        if ($?) { $result }
        else {$problems +='The SQL Call to SQLite failed'}
		if ([string]::IsNullOrEmpty($FileBasedQuery)) { Remove-Item $TempInputFile }
	}
if ($problems.Count -gt 0) { $Param1.Problems.'GetdataFromSqlite' = $problems }
}

#This is a utility scriptblock used by the task scriptblocks.
#with SQL Server, you really want your datab back as JSN, but SQLcmd can't do it.
$GetdataFromSQLCMD = {<# a Scriptblock way of accessing SQL Server via a CLI to get JSON results without having to 
explicitly open a connection. it will take SQL files and queries. It will also deal with simple SQL queries if you
set 'simpleText' to true #>
	Param ($Theargs,
		#this is the same ubiquitous hashtable 
		$query,
		#either a query that returns JSON, or a simple expression
		$fileBasedQuery = $null,
		#if you specify input from a file
		$simpleText = $false) # $GetdataFromSQLCMD: (Don't delete this)
	$problems = @();
	if ([string]::IsNullOrEmpty($TheArgs.server) -or [string]::IsNullOrEmpty($TheArgs.database))
	{ $Problems += "Cannot continue because name of either server ('$($TheArgs.server)') or database ('$($TheArgs.database)') is not provided"; }
	else
	{
		#the alias must be set to the path of your installed version of SQL Compare
        $command=$null;
        $command = get-command SQLCmd -ErrorAction Ignore 
		if ($command -eq $null)
		{
			if ($SQLCmdAlias -ne $null)
			{ Set-Alias SQLCmd   $SQLCmdAlias }
			else
			{ $problems += 'You must have provided a path to SQLcmd.exe in the ToolLocations.ps1 file in the resources folder' }
		}
		$TempOutputFile = "$($env:Temp)\TempOutput$(Get-Random -Minimum 1 -Maximum 900).json"
		if (!($simpleText))
		{
			$FullQuery = "Set nocount on; Declare @Json nvarchar(max) 
        Select @Json=($query) Select @JSON"
		}
		else
		{ $FullQuery = $query };
		if (!([string]::IsNullOrEmpty($FileBasedQuery))) #if we've been passed a file ....
		{ $TempInputFile = $FileBasedQuery }
		else
		{ $TempInputFile = "$($env:Temp)\TempInput.sql" }
		#If we can't pass a query string directly, or we have a file ...
		if ($FullQuery -like '*"*' -or (!([string]::IsNullOrEmpty($FileBasedQuery))))
		{
			#Then we can't pass a query as a string. It must be passed as a file
			#Deal with query strings 
			if ([string]::IsNullOrEmpty($FileBasedQuery)) { $FullQuery>$TempInputFile; }
			if ($TheArgs.uid -ne $null) #if we need to use credentials
			{
				sqlcmd -S $TheArgs.server -d $TheArgs.database `
					   -i $TempInputFile -U $TheArgs.Uid -P $TheArgs.pwd `
					   -o "$TempOutputFile" -u -y0
			}
			else #we are using integrated security
			{
				sqlcmd -S $TheArgs.server -d $TheArgs.database `
					   -i $TempInputFile -E -o "$TempOutputFile" -u -y0
			}
			#if it is just for storing a query
			if ([string]::IsNullOrEmpty($FileBasedQuery)) { Remove-Item $TempInputFile }
		}
		
		else #we can pass a query as a string
		{
			if ($TheArgs.uid -ne $null) #if we need to use credentials
			{
				sqlcmd -S $TheArgs.server -d $TheArgs.database `
					   -Q "`"$FullQuery`"" -U $TheArgs.uid -P $TheArgs.pwd `
					   -o `"$TempOutputFile`" -u -y0
			}
			else #we are using integrated security
			{
				sqlcmd -S $TheArgs.server -d $TheArgs.database `
					   -Q "`"$FullQuery`"" -o `"$TempOutputFile`" -u -y0
			}
		}
		If (Test-Path -Path $TempOutputFile)
		{
			#make it easier for the caller to read the error
			$response = [IO.File]::ReadAllText($TempOutputFile);
			Remove-Item $TempOutputFile
			if ($response -like 'Msg*')
			{ $Problems += "$($TheArgs.database) says $Response" }
			elseif ($response -like 'SqlCmd*')
			{ $problems += "SQLCMD says $Response" }
			
			if ($problems.count -gt 0)
			{ @{ Error = $problems } | convertTo-json }
			elseif ($response -like 'NULL*')
			{ '' }
			else
			{ $response }
		}
	}
}

$GetdataFromMySQL = {<# a Scriptblock way of accessing MySQL via a CLI to get JSON-based  results without having to 
explicitly open a connection. it will take either SQL files or queries.  #>
	Param (
		$Theargs,
		#this is the same ubiquitous hashtable 

		$query,
		#a query. If a file, put the path in the $fileBasedQuery parameter

		$fileBasedQuery = $null,
		$simpleText = $false) # $GetdataFromMySQL: (Don't delete this)
	$problems = @()
	
    $command=$null;
    $command = get-command Mysql -ErrorAction Ignore 
	if ($command -eq $null)
	{
		if ($MySQLAlias -ne $null)
		{ Set-Alias MySQL $MySQLAlias }
		else
		{ $problems += 'You must have provided a path to MySQL for $MySQLAlias in the ToolLocations.ps1 file in the resources folder' }
	}
	@('server', 'database', 'port', 'user', 'pwd') |
	foreach{ if ($TheArgs.$_ -in @($null, '')) { $problems += "Can't do this: no value for '$($_)'" } }
	
	if ($problems.Count -eq 0)
	{
		$TempInputFile = "$($env:Temp)\TempInput.sql"
		if (!([string]::IsNullOrEmpty($FileBasedQuery))) #if we've been passed a file ....
		{ $TempInputFile = $FileBasedQuery }
		else
		{ [System.IO.File]::WriteAllLines($TempInputFile, $query); }
		Try
		{
			
			$HTML = ([IO.File]::ReadAllText("$TempInputFile") | mysql "--host=$($TheArgs.server)" "--password=$($TheArgs.pwd)"  "--user=$($TheArgs.uid)" '--comments'  '--html')
			if ($? -eq 0)
			{
				$problems += "The MySQL CLI returned an error $($error[0])"
			}
			$TheColumns = @();
				$Rows = Select-String '<TR>(.*?)</TR>' -input $html -AllMatches | foreach{ $_.matches } | Foreach{
					$Col = 0;
					$lineValue = $_.Value
					
					if ($Thecolumns.count -eq 0)
					{
						$TheColumns = Select-String '<TH>(.*?)</TH>' -input $lineValue -AllMatches |
						foreach{ $_.matches.groups } | where { $_.Name -eq 1 } | foreach{ $_.value }
					}
					else
					{
						$Row = [ordered]@{ };
						Select-String '<TD>(.*?)</TD>' -input $LineValue -AllMatches |
						foreach{ $_.matches.groups } | where { $_.Name -eq 1 } | foreach{
						if ($TheColumns -is [string])
                            {$Row += @{ $TheColumns = $_.value }}
                        else
							{$Row += @{ $TheColumns[$col++] = $_.value }}
						
						}
						$Row
					}
					
				} | Where { $_.Count -gt 0 };
			}
			catch
			{ $problems += "$MySQL query failed because $($_)" }
			
			$Rows | ConvertTo-Json
			if ([string]::IsNullOrEmpty($FileBasedQuery)) {Remove-Item $TempInputFile};
		}
	if ($problems.Count -gt 0) { $Theargs.Problems.'GetdataFromMySQL' += $problems }
	}

$GetdataFromPsql = {<# a Scriptblock way of accessing PosgreSQL via a CLI to get JSON-based  results without having to 
explicitly open a connection. it will take either SQL files or queries.  #>
	Param (
        $Theargs, #this is the same ubiquitous hashtable 
		$query, #a query. If a file, put the path in the $fileBasedQuery parameter
        $fileBasedQuery=$null,
        $simpleText=$false)  # $GetdataFromPsql: (Don't delete this)
 
    $problems=@()
    $command=$null;
    $command = get-command psql -ErrorAction Ignore 
	if ($command -eq $null)
        {    if ($psqlAlias -ne $null)
        {Set-Alias psql $psqlAlias}
    else
        {$problems += 'You must have provided a path to psql in the ToolLocations.ps1 file in the resources folder'}
        }
    @('server', 'database', 'port','user','pwd') |
	        foreach{ if ($TheArgs.$_ -in @($null,'')) { $problems += "Can't do this: no value for '$($_)'" } }
    
    if ($problems.Count -eq 0)
    {
	    $TempOutputFile = "$($env:Temp)\TempOutput$(Get-Random -Minimum 1 -Maximum 900).csv"
        $TempInputFile = "$($env:Temp)\TempInput.sql"
        if (!([string]::IsNullOrEmpty($FileBasedQuery))) #if we've been passed a file ....
            {$TempInputFile=$FileBasedQuery}
        else
            {[System.IO.File]::WriteAllLines($TempInputFile, $query);}
       Try
        {
        $Params=@(
        "--dbname=$($TheArgs.database)",
        "--host=$($TheArgs.server)",
        "--username=$($TheArgs.user)",
        "--password=$($TheArgs.pwd)",
        "--port=$($TheArgs.Port -replace '[^\d]','')",
        "--file=$TempInputFile",
        '--tuples-only',
        '-Pformat=unaligned',
        "--no-password")
        $env:PGPASSWORD="$($TheArgs.pwd)"
        $result=psql @params
        }
        catch
        {$problems += "$psql query failed because $($_)"}
       if ($?)
        {$result}
       else {$problems += "The PSql CLI returned an error $($error[1])" }
    }
    if ($problems.Count -gt 0) {$Theargs.Problems.'GetdataFromPsql'+=$problems}
}


<# 
Note: now deprecated!
This scriptblock allows you to save and load the shared parameters for all these 
scriptblocks under a name you give it. you'd choose a different name for each database
within a project, but make them unique across projects 
If the parameters have the name defined but vital stuff missing, it fills 
in from the last remembered version. If it has the name and the vital stuff then it assumes
you want to save it. If no name then it ignores. 
#>
$FetchOrSaveDetailsOfParameterSet = {
	Param ($param1) # $FetchOrSaveDetailsOfParameterSet: (Don't delete this)
	$Param1.'Problems' = @{ };
	$Param1.'Warnings' = @{ };
	$Param1.'WriteLocations' = @{ };
	$problems = @();
	if ($Param1.name -ne $null -and $Param1.project -ne $null)
	{
		# define where we store our secret project details
		$StoredParameters = "$($env:USERPROFILE)\Documents\Deploy\$(($param1.Project).Split([IO.Path]::GetInvalidFileNameChars()) -join '_')";
		$ParametersFilename = 'AllParameters.json';
		$TheLocation = "$StoredParameters\$($Param1.Name)-$ParametersFilename"
	}
	$VariablesWeWant = @(
			'server', 'uid', 'port', 'project', 'database', 'projectFolder', 'projectDescription'
		);	
	if ($Param1.name -ne $null -and $Param1.project -ne $null -and $Param1.Server -eq $null -and $Param1.Database -eq $null)
	{
		# we don't want to keep passwords unencrypted and we don't want any of the transitory
		# variables that provide state (warnings, error, version number and so on)
		# If the directory doesn't exist then create it
		if (!(test-path -Path $StoredParameters -PathType Container))
		{ $null = New-Item -Path $StoredParameters -ItemType "directory" -Force }
		#if the file already exists then read it in.
		
		if (test-path $TheLocation -PathType leaf)
		{
			# so we read in all the details
			try
			{
				$JSONcontents = Get-Content -Path $TheLocation -Raw
				$TheActualParameters = $JSONcontents | ConvertFrom-json
			}
			catch
			{
				$Problems += "Cannot read file $TheLocation because it has unescaped content"
			}
			if ($problems.count -eq 0)
			{
				$TheActualParameters.psobject.properties |
				Foreach { if ($_.Name -in $VariablesWeWant) { $param1[$_.Name] = $_.Value } }
				write-verbose "fetched details from $TheLocation"
				$VariablesWeWant | where { $_ -notin @('port', 'ProjectFolder', 'projectDescription') } |
				foreach {
					if ($param1.$_ -eq $null) { $Problems += "missing a value for $($_)" }
				}
			}
		}
		else
		{
			$Problems += "Could not find project file $TheLocation"
		}
	}
	#if the user wants to save or resave they'll the name AND include both the the server and database
	elseif ($Param1.Name -ne $null -and $Param1.Server -ne $null -and $Param1.Database -ne $null)
	{
		$RememberedHashTable = @{ }
		$VariablesWeWant | foreach{ $RememberedHashTable.$_ = $param1.$_ }
		$RememberedHashTable | ConvertTo-json |
		out-file -FilePath $TheLocation -Force
		"Saved details to $TheLocation"
	}
	if ($Param1.'Checked' -eq $null) { $Param1.'Checked' = $false; }
	# has it been checked against a source directory?
	@('server', 'database', 'project') |
	foreach{ if ($param1.$_ -in @($null,'')) { $Problems += "no value for '$($_)'" } }
	if ($TheLocation -ne $null)
	{ $Param1.WriteLocations.'FetchOrSaveDetailsOfParameterSet' = $TheLocation; }
	if ($problems.Count -gt 0)
	{ $Param1.Problems.'FetchOrSaveDetailsOfParameterSet' += $problems; }
    $Param1.feedback.'FetchOrSaveDetailsOfParameterSet' += 'This FetchOrSaveDetailsOfParameterSet scriptblock is deprecated!';
}

<# now we format the Flyway parameters #>
<# Sometimes we need to run Flyway, and the easiest approach is to create a hashtable
of the information Flyway needs. It is different to the one we use for each of these
scriptblocks  #>

$FormatTheBasicFlywayParameters = {
	Param ($param1) # $FormatTheBasicFlywayParameters (Don't delete this)
	$problems = @();
	@('server', 'database', 'projectFolder') |
	   foreach{ if ($param1.$_ -in @($null,'')) { $problems += "no value for '$($_)'" } }
	$migrationsPath= if ([string]::IsNullOrEmpty($param1.migrationsPath)) {'\scripts'} else {"\$($param1.migrationsPath)"}
    if ([string]::IsNullOrEmpty($param1.Reportdirectory)){$migrationsPath=''};# compatibility problem
	$MaybePort = "$(if ([string]::IsNullOrEmpty($param1.port)) { '' }
		else { ":$($param1.port)" })"
	if (!([string]::IsNullOrEmpty($param1.uid)))
	{
		if ($param1.pwd -eq $null) { $problems += "no password provided for $($($param1.uid))" }
		$FlyWayArgs =
		@("-url=jdbc:sqlserver://$($param1.Server)$maybePort;databaseName=$($param1.Database)",
			"-WriteLocations=filesystem:$($param1.ProjectFolder)$migrationsPath", <# the migration folder #>
			"-user=$($param1.uid)",
			"-password=$($param1.pwd)")
	}
	else <# we need to use an integrated security flag in the connection string #>
	{
		$FlyWayArgs =
		@("-url=jdbc:sqlserver://$($param1.Server)$maybePort;databaseName=$(
				$param1.Database
			);integratedSecurity=true",
			"-WriteLocations=filesystem:$($param1.ProjectFolder)")<# the migration folder #>
	}
	
	$FlyWayArgs += <# the project variables that we reference with placeholders #>
	@("-placeholders.projectDescription=$($param1.ProjectDescription)",
		"-placeholders.projectName=$($param1.Project)",
		"-community") <# Change this if using teams!! #>
	$Param1.FlywayArgs = $FlyWayArgs
	if ($problems.Count -gt 0)
	{
		$Param1.Problems.'FormatTheBasicFlywayParameters' += $problems;
	}
$Param1.feedback.'FormatTheBasicFlywayParameters' += 'This FormatTheBasicFlywayParameters scriptblock is deprecated!';
}



<# This scriptblock looks to see if we have the passwords stored for this userid, Database and RDBMS
if not we ask for it and store it encrypted in the user area
 #>
$FetchAnyRequiredPasswords = {
	Param ($param1) # $FetchAnyRequiredPasswords (Don't delete this)
	$problems = @()
	try
	{
       @('server') |
		foreach{ if ($param1.$_ -in @($null, '')) { $problems += "no value for '$($_)'" } }
		# some values, especially server names, have to be escaped when used in file paths.
		if ($problems.Count -eq 0)
		{
			$escapedServer = ($Param1.server.Split([IO.Path]::GetInvalidFileNameChars()) -join '_') -ireplace '\.', '-'
			# now we get the password if necessary
			
			if (!([string]::IsNullOrEmpty($param1.uid))) #then it is using SQL Server Credentials
			{
				# we see if we've got these stored already. If specifying RDBMS, then use that.
				if ([string]::IsNullOrEmpty($param1.RDBMS))
				{ $SqlEncryptedPasswordFile = "$env:USERPROFILE\$($param1.uid)-$($escapedServer).xml" }
				else
				{ $SqlEncryptedPasswordFile = "$env:USERPROFILE\$($param1.uid)-$($escapedServer)-$($RDBMS).xml" }
				# test to see if we know about the password in a secure string stored in the user area
				if (Test-Path -path $SqlEncryptedPasswordFile -PathType leaf)
				{
					#has already got this set for this login so fetch it
					$SqlCredentials = Import-CliXml $SqlEncryptedPasswordFile
				}
				else #then we have to ask the user for it (once only)
				{
					# hasn't got this set for this login
					$SqlCredentials = get-credential -Credential $param1.uid
					# Save in the user area 
					$SqlCredentials | Export-CliXml -Path $SqlEncryptedPasswordFile
        <# Export-Clixml only exports encrypted credentials on Windows.
        otherwise it just offers some obfuscation but does not provide encryption. #>
				}
				
			$param1.Uid = $SqlCredentials.UserName;
			$param1.Pwd = $SqlCredentials.GetNetworkCredential().password
			}
        }
	}
	catch
	{
		$Param1.Problems.'FetchAnyRequiredPasswords' +=
             "$($PSItem.Exception.Message) at line $($_.InvocationInfo.ScriptLineNumber)"
	}
	if ($problems.Count -gt 0)
	{
		$Param1.Problems.'FetchAnyRequiredPasswords' += $problems;
	}
    if (!([string]::IsNullOrEmpty($param1.uid)) -and [string]::IsNullOrEmpty($param1.Pwd))
         {Write-warning "returned no password"}
}

<#This scriptblock checks the code in the database for any issues,
using SQL Code Guard to do all the work. This runs SQL Codeguard 
and saves the report in a subdirectory the version directory of your 
project artefacts. It also reports back in the $DatabaseDetails
Hashtable. It checks the current database, not the scripts
 #>
$CheckCodeInDatabase = {
	Param ($param1) # $CheckCodeInDatabase - (Don't delete this)
	#you must set this value correctly before starting.
	$Problems = @(); #our local problem counter  
    $Feedback=@(); 
    $PSDefaultParameterValues['Out-File:Encoding'] = 'utf8'; 
    $command=$null;
    $command = get-command Codeguard -ErrorAction Ignore 
	if ($command -eq $null)
	{    if ($CodeGuardAlias -ne $null)
        {Set-Alias CodeGuard   $CodeGuardAlias }
    else
        {$problems += 'You must have provided a path to CodeGuard.exe in the ToolLocations.ps1 file in the resources folder'}
        }
    try
    {
	#check that all the values we need are in the hashtable
	@('server', 'Database', 'version', 'Project') |
	foreach{ if ($param1.$_ -in @($null,'')) { $Problems += "no value for '$($_)'" } }
	#now we create the parameters for CodeGuard.
    $escapedProject=($Param1.project.Split([IO.Path]::GetInvalidFileNameChars()) -join '_') -ireplace '\.','-'
	$MyDatabasePath = 
        if  ($param1.directoryStructure -in ('classic',$null)) #If the $ReportDirectory has a value
          {"$($env:USERPROFILE)\$($param1.Reportdirectory)$($escapedProject)\$($param1.Version)\Reports"} 
        else {"$($param1.reportLocation)\$($param1.Version)\Reports"} #else the simple version
	if ($MyDatabasePath -like '*\\*'){ } { $Problems += "created an illegal path '$MyDatabasePath'" }
    $Arguments = @{
		server = $($param1.server) #The server name to connect
		Database = $($param1.database) #The database name to analyze
		outfile = "$MyDatabasePath\codeAnalysis.xml" <#
        The file name in which to store the analysis xml report#>
		#exclude='BP007;DEP004;ST001' 
		#A semicolon separated list of rule codes to exclude
		include = 'all' #A semicolon separated list of rule codes to include
	}

	#add the arguments for credentials where necessary
	if (!([string]::IsNullOrEmpty($param1.uid)))
	{
		$Arguments += @{
			User = $($param1.uid)
			Password = $($param1.pwd)
		}
	}
	# we need to make sure tha path is there
	if (-not (Test-Path -PathType Container $MyDatabasePath))
	{
		# does the path to the reports directory exist?
		# not there, so we create the directory 
		$null = New-Item -ItemType Directory -Force $MyDatabasePath;
	}
	<# we only do the analysis if it hasn't already been done for this version,
    and we've hit no problems #>
    $AlreadyDone= Test-Path -PathType leaf  "$MyDatabasePath\codeAnalysis.xml"
    if (($problems.Count -eq 0) -and (-not $alreadyDone))
	{
		$result = codeguard @Arguments; #execute the command-line Codeguard.
		if ($? -or $LASTEXITCODE -eq 1)
		{
        $CodeAnalysis =[xml] [IO.File]::ReadAllText("$MyDatabasePath\codeAnalysis.xml")
        $object=convertfrom-xml $CodeAnalysis
        $object|convertTo-json -depth 10 > "$MyDatabasePath\CodeAnalysis.json"

		$feedback += "Written Code analysis for $($param1.Project) $($param1.Version
			) to $MyDatabasePath\codeAnalysis.xml"
		}
		else
		{
			<#report a problem and send back the args for diagnosis 
            (hint, only for script development) #>
			$CLIArgs = '';
			$CLIArgs += $Arguments |
			foreach{ "$($_.Name)=$($_.Value)" }
			$problems += "CodeGuard responded '$result' with error code $LASTEXITCODE when used with parameters $CLIArgs."
		}
		#$Problems += $result | where { $_ -like '*error*' }
	}
}
	catch
	{
		$Param1.Problems.'CheckCodeInDatabase' +=
             "$($PSItem.Exception.Message) at line $($_.InvocationInfo.ScriptLineNumber)"
	}

	if ($problems.Count -gt 0)
	{
		$Param1.Problems.'CheckCodeInDatabase' += $problems;
	}
	else
    {
    if ($AlreadyDone) {$Param1.feedback.'CheckCodeInDatabase'="There is already a database code check for version $($param1.version)"}
    else 
        { $Param1.feedback.'CheckCodeInDatabase' += $Feedback }   
    $Param1.WriteLocations.'CheckCodeInDatabase' = "$MyDatabasePath\codeAnalysis.xml";}
}


<#This scriptblock checks the code in the migration files for any issues,
using SQL Code Guard to do all the work. This runs SQL Codeguard 
and saves the report in a subdirectory the version directory of your 
project artefacts. It also reports back in the $DatabaseDetails
Hashtable. It checks the scripts not the current database.
#>

$CheckCodeInMigrationFiles = {
	Param ($param1) # $CheckCodeInMigrationFiles - (Don't delete this)
	#you must set this value correctly before starting.
	$Problems = @(); #our local problem counter
    $Feedback = @();
    $PSDefaultParameterValues['Out-File:Encoding'] = 'utf8';
	$command=$null;
    $command = get-command Codeguard -ErrorAction Ignore 
    if ($command -eq $null) 
	{    if ($CodeGuardAlias -ne $null)
        {Set-Alias CodeGuard   $CodeGuardAlias }
    else
        {$problems += 'You must have provided a path to CodeGuard.exe in the ToolLocations.ps1 file in the resources folder'}
        }
	#check that all the values we need are in the hashtable
	@('ProjectFolder') |
	foreach{ if ($param1.$_ -in @($null,'')) { $Problems += "no value for '$($_)'" } }
    $escapedProject=($Param1.project.Split([IO.Path]::GetInvalidFileNameChars()) -join '_') -ireplace '\.','-'

	#$migrationsPath= if ([string]::IsNullOrEmpty($param1.migrationsPath)) {'\scripts'} else {"\$($param1.migrationsPath)"}
	#if ([string]::IsNullOrEmpty($param1.Reportdirectory)){$migrationsPath=''};# compatibility problem

    dir "$($param1.projectFolder)\V*.sql" | foreach{
		$Thepath = $_.FullName;
		$TheFile = $_.Name
		$Theversion = ($_.Name -replace 'V(?<Version>[.\d]+).+', '${Version}')
        if ([version]$Theversion -le [version]$Param1.version)
            {
		    #now we create the parameters for CodeGuard.
		    $MyVersionReportPath = 
              if  ($param1.directoryStructure -in ('classic',$null)) #If the $ReportDirectory has a value
                {"$($env:USERPROFILE)\$($param1.Reportdirectory)$($escapedProject)\$($param1.Version)\Reports"} 
              else {"$($param1.reportLocation)\$($param1.Version)\Reports"} #else the simple version
		    $Arguments = @{
			    source = $ThePath
			    outfile = "$MyVersionReportPath\FilecodeAnalysis.xml" <#
                The file name in which to store the analysis xml report#>
			    #exclude='BP007;DEP004;ST001' 
			    #A semicolon separated list of rule codes to exclude
			    include = 'all' #A semicolon separated list of rule codes to include
		    }
		    # we need to make sure tha path is there
		    if (-not (Test-Path -PathType Container $MyVersionReportPath))
		    {
			    # does the path to the reports directory exist?
			    # not there, so we create the directory 
			    $null = New-Item -ItemType Directory -Force $MyVersionReportPath;
		    }
            $alreadyDone=Test-Path -PathType leaf  "$MyVersionReportPath\FilecodeAnalysis.xml"
	        <# we only do the analysis if it hasn't already been done for this version,
            and we've hit no problems #>
		    if (($problems.Count -eq 0) -and (-not ( $alreadyDone)))
		    {
			    $result = codeguard @Arguments; #execute the command-line Codeguard.
			    if ($? -or $LASTEXITCODE -eq 1)
			    {
                $CodeAnalysis =[xml] [IO.File]::ReadAllText("$MyVersionReportPath\FilecodeAnalysis.xml")
                $object=convertfrom-xml $CodeAnalysis
                $object|convertTo-json -depth 10 > "$MyVersionReportPath\FilecodeAnalysis.json"
				$feedback += "Written file Code analysis for $TheFile for $($param1.Project) project) to $MyVersionReportPath\FilecodeAnalysis.xml and .json"
			    }
			    else
			    {
			        <#report a problem and send back the args for diagnosis 
                    (hint, only for script development) #>
				    $CLIArgs = '';
				    $CLIArgs += $Arguments.GetEnumerator() |
				    foreach{ "$($_.Name)=$($_.Value)" }
				    $problems += "CodeGuard responded '$result' with error code $LASTEXITCODE when used with parameters $CLIArgs."
			    }
			    #$Problems += $result | where { $_ -like '*error*' }
		    }
        }
	}
	if ($problems.Count -gt 0)
	{
		$Param1.Problems.'CheckCodeInMigrationFiles' += $problems;
	}
    else
    {
    if ($AlreadyDone) {$Param1.feedback.'CheckCodeInMigrationFiles'="There is already a database code check for version $($param1.version)"}
    else 
        { $Param1.feedback.'CheckCodeInMigrationFiles' += $Feedback }   

    $Param1.WriteLocations.'CheckCodeInMigrationFiles' = "$MyVersionReportPath\FilecodeAnalysis.xml"; }
	$Param1.feedback.'CheckCodeInMigrationFiles' += $Feedback
		
}


<#This scriptblock gets the current version of a flyway_schema_history data from the 
table in the database. if there is no Flyway Data, then it returns a version of 0.0.0
 #>
$GetCurrentVersion = {
	Param ($param1) # $GetCurrentVersion parameter is a hashtable 
	$problems = @();
	$doit = $true;
	@('server', 'rdbms', 'database') | foreach{
		if ($param1.$_ -in @($null, ''))
		{
			$Problems += "no value for '$($_)'";
			$DoIt = $False;
		}
	}
	$flywayTable = $Param1.flywayTable
	if ($flywayTable -eq $null)
	{ $flywayTable = 'dbo.flyway_schema_history' }
	$Version = 'unknown'
	if ($param1.RDBMS -eq 'sqlserver')
	{
		# Do it the SQL Server way.
		$AllVersions = $GetdataFromSQLCMD.Invoke(
			$param1, "SELECT DISTINCT version
      FROM $flywayTable
      WHERE version IS NOT NULL
    FOR JSON AUTO") |
		convertfrom-json
		$LastAction = $GetdataFromSQLCMD.Invoke(
			$param1, "SELECT version, type
      FROM $flywayTable
      WHERE
      installed_rank =
        (SELECT Max (installed_rank) FROM $flywayTable
           WHERE success = 1)
    FOR JSON AUTO") |
		convertfrom-json
	} ## we do it the postgresql way
	elseif ($param1.RDBMS -eq 'postgresql')
	{
		# Do it the PostgreSQL way
		$AllVersions = Execute-SQL $param1  "
            SELECT json_agg(e) 
            FROM (SELECT DISTINCT version
      FROM $($param1.flywayTable)
      WHERE version IS NOT NULL
      )e;          
    " | convertfrom-json
		$LastAction = Execute-SQL $param1 "
      SELECT json_agg(e) 
            FROM (SELECT version, type
      FROM $($param1.flywayTable)
      WHERE
      installed_rank =
        (SELECT Max (installed_rank) FROM $($param1.flywayTable)
           WHERE success = true))e;
    " | convertfrom-json
	} ## OK, lets do it the SQLite way.
	elseif ($param1.RDBMS -eq 'sqlite')
	{
		# Do it the SQLite way
		$AllVersions = $GetdataFromsqlite.Invoke(
			$param1, "SELECT DISTINCT version
      FROM $($param1.flywayTable)
      WHERE version IS NOT NULL
    ") | convertfrom-json
		$LastAction = $GetdataFromSqlite.Invoke(
			$param1, "SELECT version, type
      FROM $($param1.flywayTable)
      WHERE
      installed_rank =
        (SELECT Max (installed_rank) FROM $($param1.flywayTable)
           WHERE success = 1)
    ") | convertfrom-json
	}
    elseif ($param1.RDBMS -in @('mysql','mariadb'))
	{
		# Do it the MySQL way
		$AllVersions = Execute-SQL $param1  "
        SELECT DISTINCT version
        FROM $($param1.flywayTable)
      WHERE version IS NOT NULL          
    " | convertfrom-json
		$LastAction = Execute-SQL $param1 "
      SELECT version, type
      FROM $($param1.flywayTable)
      WHERE
      installed_rank =
        (SELECT Max(installed_rank) FROM $($param1.flywayTable)
           WHERE success = true);
    " | convertfrom-json
	} 
	else { $problems += "$($param1.RDBMS) is not supported yet. " }
	if ($AllVersions.error -ne $null) { $problems += $AllVersions.error }
	if ($LastAction.error -ne $null) { $problems += $LastAction.error }
	if ($AllVersions -eq $null) { $problems += 'No response for version list' }
	if ($LastAction -eq $null) { $problems += 'no response for last migration' }
    $version = '0.0.0'; $previous = '0.0.0'
	if ($problems.count -eq 0)
	{
		$OrderedVersions = $AllVersions | %{
			new-object System.Version ($_.version)
		} |
		sort | % -Begin { $ii = 1 }{
			[pscustomobject]@{ 'Order' = $ii++; 'version' = $_.ToString() }
		};
		
		$VersionOrder = $OrderedVersions |
		where{ $_.version -eq $Lastaction.version } | Select Order -First 1;
		$previous = ($OrderedVersions |
			where{ $_.Order -eq $VersionOrder.Order - 1 } |
			Select version -First 1).version;
        if ($previous -eq $null) {$previous = '0.0.0'}
		$version = $LastAction.version;
		if ($LastAction.type -like 'UNDO*')
		{
			$param1.Version = $previous;
			$param1.Previous = $Version;
		}
		else
		{
			$param1.Version = $version;
			$param1.Previous = $previous;
		}
	}
	if ($problems.Count -gt 0)
	{
		$Param1.Problems.'GetCurrentVersion' += $problems;
	}
	$Param1.feedback.'GetCurrentVersion' = "current version is $version, previous $previous."
}


<#This scriptblock gets the current version of the RDBMS on the server
and is used mainly to check that the migration doesn't use any
functionality that can't be supported on that server version. It updates
the $Param1.ServerVersion 
 #>
$GetCurrentServerVersion = {
	Param ($param1) # $GetCurrentServerVersion parameter is a hashtable 
	$problems = @();
	$doit = $true;
	@('server', 'rdbms') | foreach{
		if ($param1.$_ -in @($null, ''))
		{
			$Problems += "no value for '$($_)'";
			$DoIt = $False;
		}
	}
	switch -Regex ($param1.RDBMS)
	{
		'sqlserver'   {
			$Version = Execute-SQL $param1 @'
SELECT 'SQL20'+ CASE Left(Cast(ServerProperty ('productversion') AS VARCHAR(80)), 
				CharIndex ('.', Cast(ServerProperty ('productversion') AS VARCHAR(80))))
		 WHEN '8.'  THEN '00'
         WHEN '9.'  THEN '05'
         WHEN '10.' THEN '08'
         WHEN '11.' THEN '12'
         WHEN '12.' THEN '14'
         WHEN '13.' THEN '16'
         WHEN '14.' THEN '17'
         WHEN '15.' THEN '19' ELSE 'xx?' END AS Version,
       ServerProperty ('ProductLevel') AS ProductLevel,
       ServerProperty ('Edition') AS Edition,
       ServerProperty ('ProductVersion') AS ProductVersion
FOR JSON PATH
'@ | Convertfrom-json
			$Param1.ServerVersion = $Version.version
		}
		'postgresql'
		{
			# Do it the PostgreSQL way
			$Version = Execute-SQL $param1 @'
SELECT json_agg(e) FROM (SELECT Version() as "DatabaseVersion")e;
'@ | Convertfrom-json
			$Param1.ServerVersion = $Version.DatabaseVersion
		}
		'sqlite'
		{
			## OK, lets do it the SQLite way.
			$Version = Execute-SQL $param1 @'
select sqlite_version() as version;
'@ | Convertfrom-json
			$Param1.ServerVersion = $Version.version
		}
		'mysql|mariadb'
		{
			#get MariaDB version
			$Version = Execute-SQL $param1 @'
SHOW VARIABLES LIKE "version"; ;
'@ | Convertfrom-json
			$Param1.ServerVersion = $Version.value
		}
		Default { $problems += "$($param1.RDBMS) is not supported yet. " }
	}
	if ($problems.Count -gt 0)
	{
		$Param1.Problems.'$GetCurrentServerVersion' += $problems;
	}
	else
	{
		$Param1.feedback.'$GetCurrentServerVersion' = "current $($param1.RDBMS) version is $($Param1.ServerVersion)."
	}
}



<# This uses SQL Compare to check that a version of a database is correct and hasn't been changed.
It returns comparison equal to true if it was the same or false if there has been drift, 
with a list of objects that have changed. if the comparison returns $null, then it means there 
has been an error #>

$IsDatabaseIdenticalToSource = {
	Param ($param1) # $IsDatabaseIdenticalToSource (Don't delete this)
	$problems = @();
	$warnings = @();
	@('version', 'server', 'database', 'project') |
	foreach{ if ($param1.$_ -in @($null, '')) { $problems += "no value for '$($_)'" } }
	if ($param1.Version -eq '0.0.0') { $identical = $null; $warnings += "Cannot compare an empty database" }
	$GoodVersion = try { $null = [Version]$param1.Version; $true }
	catch { $false }
	if (-not ($goodVersion))
	{ $problems += "Bad version number '$($param1.Version)'" }
	#the alias must be set to the path of your installed version of SQL Compare
	$command = $null;
	$command = get-command SQLCompare -ErrorAction Ignore
	if ($command -eq $null)
	{
		if ($SQLCompareAlias -ne $null)
		{ Set-Alias SQLCompare $SQLCompareAlias; }
		else
		{ $problems += 'You must have provided a path to SQL Compare in the ToolLocations.ps1 file in the resources folder' }
	}
	$escapedProject = ($Param1.Project.Split([IO.Path]::GetInvalidFileNameChars()) -join '_') -ireplace '\.', '-'
	if ($problems.Count -eq 0)
	{
		
		$SourcePath = if ([string]::IsNullOrEmpty($param1.SourcePath)) { 'Source' }
		else { "$($param1.SourcePath)" }
		#the database scripts path would be up to you to define, of course
		$MyDatabasePath =
		if ($param1.directoryStructure -in ('classic', $null)) #If the $ReportDirectory has a value
		{ "$($env:USERPROFILE)\$($param1.Reportdirectory)$($escapedProject)\$($param1.Version)\$sourcePath" }
		else { "$($param1.reportLocation)\$($param1.Version)\$sourcePath" } #else the simple version
		$CLIArgs = @(# we create an array in order to splat the parameters. With many command-line apps you
			# can use a hash-table 
			"/Scripts1:$MyDatabasePath",
			"/server2:$($param1.server)",
			"/database2:$($param1.database)",
			"/Assertidentical",
			"/force",
			"/LogLevel:Warning"
		)
		
		if ($param1.uid -ne $NULL) #add the arguments for credentials where necessary
		{
			$CLIArgs += @(
				"/username2:$($param1.uid)",
				"/Password2:$($param1.pwd)"
			)
		}
		if ($param1.'filterpath' -ne $NULL) #add the arguments for compare filters
		{
			$CLIArgs += @(
				"/filter:$($param1.filterpath)"
			)
		}
		else
		{
			$CLIArgs += @(
				"/exclude:table:$($param1.flywayTable)",
				'/exclude:ExtendedProperty') #trivial}
		}
	}
	$MyVersionReportPath =
	if ($param1.directoryStructure -in ('classic', $null)) #If the $ReportDirectory has a value
	{ "$($env:USERPROFILE)\$($param1.Reportdirectory)$($escapedProject)\$($param1.Version)\Reports" }
	else { "$($param1.reportLocation)\$($param1.Version)\Reports" } #else the simple version
	if ($problems.Count -eq 0)
	{
		if (Test-Path -PathType Container $MyDatabasePath) #if it does already exist
		{
			Sqlcompare @CLIArgs >"$MyVersionReportPath\VersionComparison.txt" #simply check that the two are identical
			if ($LASTEXITCODE -eq 0) { $identical = $true; "Database Identical to source" }
			elseif ($LASTEXITCODE -eq 79) { $identical = $False; "Database Different to source" }
			else
			{
				#report a problem and send back the args for diagnosis (hint, only for script development)
				$Arguments = ''; $identical = $null;
				$Arguments += $CLIArgs | foreach{ $_ }
				$problems += "That Went Badly (code $LASTEXITCODE) with paramaters $Arguments."
			}
		}
		else
		{
			$identical = $null;
			$Warnings = "source folder '$MyDatabasePath' did not exist so can't check"
		}
		
	}
	$param1.Checked = $identical
	if ($problems.Count -gt 0)
	{ $Param1.Problems.'IsDatabaseIdenticalToSource' += $problems; }
	else
	{
		$Param1.WriteLocations.'IsDatabaseIdenticalToSource' = "$MyVersionReportPath\VersionComparison.txt";
	}
	if ($warnings.Count -gt 0)
	{ $Param1.Warnings.'IsDatabaseIdenticalToSource' += $Warnings; }
}

<#this routine checks to see if a script folder already exists for this version
of the database and, if not, it will create one and fill it with subdirectories
for each type of object. A tables folder will, for example, have a file for every table
each containing a  build script to create that object.
When this exists, it allows SQL Compare to do comparisons and check that a version has not
drifted.
$param1=$dbdetails  #>
$CreateScriptFoldersIfNecessary = {
	Param ($param1) # $CreateScriptFoldersIfNecessary 
	$Problems = @(); #We check that it contains the keys for the values that we need 
	$freedback = @();
	$PSDefaultParameterValues['Out-File:Encoding'] = 'utf8'
	@('version', 'server', 'rdbms', 'database', 'project') |
	foreach{ if ($param1.$_ -eq $null) { $problems += "no key for the '$($_)'" } }
	
	#the database scripts path would be up to you to define, of course
	$EscapedProject = ($Param1.Project.Split([IO.Path]::GetInvalidFileNameChars()) -join '_') -ireplace '\.', '-'
	$SourcePath = if ([string]::IsNullOrEmpty($param1.SourcePath)) { 'Source' }
	else { "$($param1.SourcePath)" }
	$MyDatabasePath =
	if ($param1.directoryStructure -in ('classic', $null)) #If the $ReportDirectory has a value
	{ "$($env:USERPROFILE)\$($param1.Reportdirectory)$($escapedProject)\$($param1.Version)\$sourcePath" }
	else { "$($param1.reportLocation)\$($param1.Version)\$sourcePath" } #else the simple version
	$MyCurrentPath =
	if ($param1.directoryStructure -in ('classic', $null)) #If the $ReportDirectory has a value
	{ "$($env:USERPROFILE)\$($param1.Reportdirectory)$($escapedProject)\current\$sourcePath" }
	else { "$($param1.reportLocation)\current\$sourcePath" } #else the simple version
	if (!(Test-Path -PathType Container $MyDatabasePath))
	{
		switch -Regex ($param1.RDBMS)
		{
			'sqlserver' #using SQL Server
			{
				$command=$null;
                $command = get-command SQLCompare -ErrorAction Ignore 
                if ($command -eq $null) 
				{
					if ($SQLCompareAlias -ne $null)
					{ Set-Alias SQLCompare $SQLCompareAlias }
					else
					{ $problems += 'You must have provided a path to SQL Compare in the ToolLocations.ps1 file in the resources folder' }
				}
				$CLIArgs = @(
					"/server1:$($param1.server)",
					"/database1:$($param1.database)",
					"/Makescripts:$($MyDatabasePath)", #special command to make a scripts directory
					"/force",
					"/LogLevel:Warning"
				)
				
				if ($param1.uid -ne $NULL) #add the arguments for credentials where necessary
				{
					$CLIArgs += @(
						"/username1:$($param1.uid)",
						"/Password1:$($param1.pwd)"
					)
				}
				if ($param1.'filterpath' -ne $NULL) #add the arguments for compare filters
				{
					$CLIArgs += @(
						"/filter:$($param1.filterpath)"
					)
				}
				else
				{
					$CLIArgs += @(
						"/exclude:table:$($param1.flywayTable)",
						'/exclude:ExtendedProperty') #trivial}
				}
				
				if ($problems.Count -eq 0) #if it doesn't already erxist
				{
					Sqlcompare @CLIArgs #write an object-level  script folder that represents the vesion of the database
					if (!($?))
					{
						#report a problem and send back the args for diagnosis (hint, only for script development)
						$problems += "SQL Compare responded with error code $LASTEXITCODE "
					}
				}
			}
			'mariadb|mysql' #--do it with MySQL or MariaDB
			{
				$command=$null;
                $command = get-command mysqldump -ErrorAction Ignore 
                if ($command -eq $null)
				{
					if ($mysqldumpAlias -ne $null)
					{ Set-Alias mysqldump $mysqldumpAlias  }
					else
					{ $problems += 'You must have provided a path to mysqldump in the $mysqldumpAlias ToolLocations.ps1 file in the resources folder' }
				}

				$ObjectList = Execute-SQL $param1 @'
        SELECT t.Table_Schema AS "Schema", t.TABLE_NAME AS "Name", 
          case when r.Routine_NAME IS NOT NULL 
            then LOWER(r.routine_type) 
            ELSE lower(replace(t.Table_type,'BASE','')) END AS "Type" 
        FROM information_schema.tables t
        LEFT OUTER JOIN information_schema.routines r
        oN r.routine_Schema=t.table_schema
        AND r.routine_Name=t.table_Name
        WHERE table_schema='dbo' AND TABLE_NAME <>'flyway_schema_history'
'@
				$object = $ObjectList | Convertfrom-json
				$object | foreach{
					$WhereToStoreIt = "$MyDatabasePath\$($_.Type.Trim())"
					if ($problems.Count -eq 0)
					{
						if (-not (Test-Path "$WhereToStoreIt" -PathType Container))
						{ $Null = New-Item -ItemType directory -Path "$WhereToStoreIt" -Force }
						mysqldump "--host=$($param1.server)" "--password=$($param1.pwd)" "--user=$($param1.uid)" --triggers --skip-set-charset --skip-add-drop-table --skip-set-charset --compact --no-data  "$($_.Schema)" "$($_.Name)" > "$WhereToStoreIt\$($_.Schema).$($_.Name).sql"
						if (!($?))
						{
							#report a problem and send back the args for diagnosis
							$problems += "mysqldump responded with error code $LASTEXITCODE "
						}
					}
				}
			}
			'postgresql'
			{
				$command=$null;
                $command = get-command pg_dump -ErrorAction Ignore 
				if ($command -eq $null)
				{
					if ($PGDumpAlias -ne $null)
					{ Set-Alias pg_dump   $PGDumpAlias; }
					else
					{
						$problems += 'You must have provided a path to pg_dump.exe in the ToolLocations.ps1 file in the resources folder'
					}
				}
				$env:PGPASSWORD = "$($param1.pwd)"
				if (-not (Test-Path "$MyDatabasePath" -PathType Container))
				{ $null = New-Item -ItemType directory -Path "$MyDatabasePath" -Force }
				$Params = @(
					"--dbname=$($param1.database)",
					"--host=$($param1.server)",
					"--username=$($param1.user)",
					"--file=$MyDatabasePath\FullBuild.sql",
					"--port=$($param1.Port -replace '[^\d]', '')",
					'--encoding=UTF8',
					'--schema-only'
				)
				pg_dump @Params
				if (!($?))
				{
					#report a problem and send back the args for diagnosis (hint, only for script development)
					$problems += "pg_dump responded with error code $LASTEXITCODE "
				}
				#Read in the build script 
				$FullBuild = [IO.File]::ReadAllText("$MyDatabasePath\FullBuild.sql")
				#Each statement has a header with useful information. This saves a lot of work, so we parse it
				$Regex = '-- Name: (?<name>.+); Type: (?<type>.+); Schema: (?<schema>.+); Owner: (?<owner>.+)\n(?<contents>(?s:.)+?)(?=(-- Name|-- PostgreSQL))'
				$TheObjects = @() #We hold the list of base objects (Tables, views, procedures)
				$TheChildren = @() #We hold the list of child objects (constraints, indexes etc)
				Select-String $regex -input $FullBuild -AllMatches | Foreach { $_.Matches } | foreach{
					$match = $_
					$name = $match.Groups[2].Value;
					$type = $match.Groups[3].Value;
					$schema = $match.Groups[4].Value;
					$owner = $match.Groups[5].Value;
					$contents = $match.Groups[6].Value;
					if ($contents -match 'ALTER TABLE ONLY (?<tablename>.+)') #then they are child objects
					{
						$TheMatch = $matches;
						$TheChildren += @{ 'Name' = $TheMatch.tablename; 'contents' = $contents }
					}
					elseif ($type -eq 'INDEX')
					{
						if ($contents -imatch 'CREATE INDEX .+ ON (["a-z \._]+) USING')
						{
							$TheChildren += @{ 'Name' = $Matches.1; 'contents' = $contents }
						}
					}
					else #we assume that they are base objects
					{
						$TheObjects += @{ 'Name' = $Name; 'schema' = $schema; 'Type' = $type; 'contents' = $contents }
					}
				}
				$TheObjects | foreach{
					$Object_name = "$($_.schema).$($_.name)";
					$Escaped_Object_name = "$($_.schema).`"$($_.name)`"";
					$_.contents += $TheChildren |
					where { $_.name -in @($Object_name, $Escaped_Object_name) } |
					foreach { $code = '' }{ $code += "$($_.contents)" }{ $code }
				}
				$TheObjects | foreach{
					$SchemaToStoreIt = "$MyDatabasePath\$($_.Type.ToLower())" #store it according to type
					if (-not (Test-Path "$SchemaToStoreIt" -PathType Container)) #make sure exzists
					{ New-Item -ItemType directory -Path "$SchemaToStoreIt" -Force }
					$_.Contents > "$SchemaToStoreIt\$($_.schema.ToLower()).$($_.name).sql"; #pop it into the file
				}
			}
			'sqlite'
			{
				$command=$null;
                $command = get-command sqlite -ErrorAction Ignore 
				if ($command -eq $null)
				{
					if ($sqliteAlias -ne $null)
					{ Set-Alias sqlite $sqliteAlias  }
					else
					{ $problems += 'You must have provided a path to sqlite.exe in the ToolLocations.ps1 file in the resources folder' }
				}
				
				$params = @(
					'.bail on',
					'.mode json',
					'.headers off',
					"SELECT name,type FROM sqlite_schema 
                    WHERE type IN ('table','view') and name NOT LIKE 'sqlite_%' 
                    -- and name not like '$($param1.flywayTable)%' 
                    ORDER BY 1",
					'.quit')
				try {
                $TableList = sqlite "$($param1.database)" @Params
				}
		        catch
		        { $problems += "SQL called to SQLite to get list of objects failed because $($_)" }
                if (!($?)) {$problems +='The SQL Call to  get list of objects  from SQLite failed'}
				$Tables = $TableList | convertFrom-json
				$Scripts = $Tables | foreach {
					$params = @(
						'.bail on',
						".schema $($_.name)",
						'.quit');
					@{ 'name' = $_.name; 'type' = $_.type; 'script' = sqlite "$($param1.database)" @Params; };
				}
				$scripts | foreach{
					$SchemaToStoreIt = "$MyDatabasePath\$($_.Type.ToLower())" #store it according to type
					if (-not (Test-Path "$SchemaToStoreIt" -PathType Container)) #make sure exzists
					{ $null = New-Item -ItemType directory -Path "$SchemaToStoreIt" -Force }
					$_.script > "$SchemaToStoreIt\$($_.name).sql"; #pop it into the file
				}
			}
			default
			{ $problems += "Sorry but a script folder isn''t supported from your RDBMS $($param1.RDBMS)" }
		}
		if ($feedback.count -gt 0)
		{ $Param1.feedback.'CreateScriptFoldersIfNecessary' = $feedback }
		if ($problems.count -gt 0)
		{ $Param1.problems.'CreateScriptFoldersIfNecessary' = $problems }
		else
		{
			$Param1.WriteLocations.'CreateScriptFoldersIfNecessary' = "$MyDatabasePath";
            if (-not (Test-Path "$MyCurrentPath" -PathType Container))
                { $null=New-Item -ItemType directory -Path "$MyCurrentPath" -Force}
            Remove-Item "$MyCurrentPath\*" -Recurse 
			copy-item -path  "$MyDatabasePath\*" -recurse -destination "$MyCurrentPath" -Container # copy over the current model
			@{ 'version' = $Param1.version; 'Author' = $Param1.InstalledBy; 'Branch' = $param1.branch } |
			convertTo-json >"$(split-path -path $MyCurrentPath -parent)\Version.json"
		}
	}
	else { $Param1.feedback.'CreateScriptFoldersIfNecessary' = "This version ($($param1.Version)) is already scripted in $MyDatabasePath " }
}

<# 
a script block that produces a build script from a database, using SQL Compare, pg_dump or whatever.
$param1=$dbDetails #>

$CreateBuildScriptIfNecessary = {
	Param ($param1) # $CreateBuildScriptIfNecessary (Don't delete this) 
	$problems = @();
    $PSDefaultParameterValues['Out-File:Encoding'] = 'utf8'
	@('version', 'server', 'database', 'project') |
	foreach{ if ($param1.$_ -in @($null, '')) { $Problems += "no value for '$($_)'" } }

	#the database scripts path would be up to you to define, of course
	$EscapedProject = ($Param1.project.Split([IO.Path]::GetInvalidFileNameChars()) -join '_') -ireplace '\.', '-'
	$scriptsPath = if ([string]::IsNullOrEmpty($param1.scriptsPath)) { 'scripts' }
	else { "$($param1.scriptsPath)" }
	if ($param1.directoryStructure -in ('classic', $null)) #If the $ReportDirectory has a value
	    {$MyDatabasePath = "$($env:USERPROFILE)\$($param1.Reportdirectory)$($escapedProject)\$($param1.Version)\$scriptsPath";
         $MyCurrentPath = "$($env:USERPROFILE)\$($param1.Reportdirectory)$($escapedProject)\current\$scriptsPath";}
	else {$MyDatabasePath = "$($param1.reportLocation)\$($param1.Version)\$scriptsPath";
          $MyCurrentPath = "$($param1.reportLocation)\current\$scriptsPath"; } #else the simple version
	
	if (-not (Test-Path -PathType Leaf "$MyDatabasePath\V$($param1.Version)__Build.sql"))
	{
		if (-not (Test-Path -PathType Container $MyDatabasePath))
		{
			# is the path to the scripts directory
			# not there, so we create the directory 
			$null = New-Item -ItemType Directory -pa $MyDatabasePath -Force;
		}
		if (-not (Test-Path -PathType Container $MyCurrentPath))
		{
			# is the path to the current directory
			# not there, so we create the directory 
			$null = New-Item -Path $MyCurrentPath -ItemType "directory" -Force;
		}
		switch -Regex ($param1.RDBMS)
		{
			'sqlserver' #using SQL Server
			{
            	#the alias must be set to the path of your installed version of SQL Compare
			    $command = get-command SQLCompare -ErrorAction Ignore 
                if ($command -eq $null) 
                    {
    	            if ($SQLCompareAlias-ne $null)
                        {Set-Alias SQLCompare $SQLCompareAlias}
                    else
                        {$problems += 'You must have provided a path to SQL Compare in the ToolLocations.ps1 file in the resources folder'}
                }
				$CLIArgs = @(# we create an array in order to splat the parameters. With many command-line apps you
					# can use a hash-table 
					"/server1:$($param1.server)",
					"/database1:$($param1.database)",
					"/empty2",
					"/force", # 
					"/options:NoTransactions,NoErrorHandling", # so that we can use the script with Flyway more easily
					"/LogLevel:Warning",
					"/ScriptFile:$MyDatabasePath\V$($param1.Version)__Build.sql"
				)
				
				if ($param1.uid -ne $NULL) #add the arguments for credentials where necessary
				{
					$CLIArgs += @(
						"/username1:$($param1.uid)",
						"/Password1:$($param1.pwd)"
					)
				}
                if ($param1.'filterpath' -ne $NULL) #add the arguments for compare filters
		        {
			        $CLIArgs += @(
				        "/filter:$($param1.filterpath)"
			        )
                 }
                else
                    {
                    $CLIArgs += @(
                      "/exclude:table:$($param1.flywayTable)")
		        } 
				
				# if it is done already, then why bother? (delete it if you need a re-run for some reason 	
				Sqlcompare @CLIArgs #run SQL Compare with splatted arguments
				if ($?) { $Param1.WriteLocations.'CreateBuildScriptIfNecessary'="Written build script for $($param1.Project) $($param1.Version) to $MyDatabasePath"
                Copy-Item -Path "$MyDatabasePath\V$($param1.Version)__Build.sql" -Destination "$MyCurrentPath\current__Build.sql"
                }
				else # if no errors then simple message, otherwise....
				{
					#report a problem and send back the args for diagnosis (hint, only for script development)
					$Arguments = '';
					$Arguments += $CLIArgs | foreach{ $_ }
					$Problems += "SQLCompare Went badly. (code $LASTEXITCODE) with paramaters $Arguments."
				}
				break;
			}
			'postgresql' #using SPostgreSQL
			{
            	#the alias must be set to the path of your installed version of Spg_dump
             $command=$null;
             $command = get-command pg_dump -ErrorAction Ignore;
	         if ($command -eq $null)               
                {if ($PGDumpAlias -ne $null)
                    {Set-Alias pg_dump   $PGDumpAlias;}
                else
                    {$problems += 'You must have provided a path to pg_dump.exe in the ToolLocations.ps1 file in the resources folder'
                    }
                }
                $env:PGPASSWORD="$($param1.pwd)"
                $Params=@(
                    "--dbname=$($param1.database)",
                    "--host=$($param1.server)",
                    "--username=$($param1.user)",
                    "--port=$($param1.Port -replace '[^\d]','')",
                    "--file=$MyDatabasePath\V$($param1.Version)__Build.sql",
                    '--encoding=UTF8',
                    '--schema-only')
                 pg_dump @Params 
				if ($?) 
                { $Param1.feedback.'CreateBuildScriptIfNecessary'="Written PG build script for $($param1.Project) $($param1.Version) to $MyDatabasePath" 
                Copy-Item -Path "$MyDatabasePath\V$($param1.Version)__Build.sql" -Destination "$MyCurrentPath\current__Build.sql" -Force
                }
				else # if no errors then simple message, otherwise....
				{
					$Problems += "pg_dump Went badly. (code $LASTEXITCODE)"
				}
				break;
			}
            'mysql|mariadb'
            {
                $command=$null;
                $command = get-command mysqldump -ErrorAction Ignore 
                
	            if ($command -eq $null)
	                {if ($MySQLDumpAlias -ne $null)
                        {Set-Alias mysqldump $MySQLDumpAlias }
                    else
                        {$problems += 'You must have provided a path to mysqldump.exe in $MySQLDumpAlias the ToolLocations.ps1 file in the resources folder'}
                    }            else
                {
                powershell.exe "mysqldump --host=$($param1.server) --password=$($param1.pwd) --user=$($param1.uid) --no-data --databases $($param1.schemas -replace ',',' ')" > "$MyDatabasePath\V$($param1.Version)__Build.sql"
                Copy-Item -Path "$MyDatabasePath\V$($param1.Version)__Build.sql" -Destination "$MyCurrentPath\current__Build.sql" -Force
               } 
            }
            'sqlite' #using SQLite
            {
                $command=$null;
                $command = get-command sqlite -ErrorAction Ignore 
	            if ($command -eq $null)
	                {if ($sqliteAlias -ne $null)
                        {Set-Alias sqlite $sqliteAlias }
                    else
                        {$problems += 'You must have provided a path to sqlite.exe in the ToolLocations.ps1 file in the resources folder'}
                    }
         		$params = @(
			        '.bail on',
                    '.schema',
			        '.quit')
		        try
		        {
			        sqlite "$($param1.database)" @Params > "$MyDatabasePath\V$($param1.Version)__Build.sql"
		        }		
                catch
		        { $problems += "SQL called to SQLite  failed because $($_)"
                }           
                if ($?)
                { $Param1.feedback.'CreateBuildScriptIfNecessary'="Written SQLite build script for $($param1.Project)" 

                Copy-Item -Path "$MyDatabasePath\V$($param1.Version)__Build.sql" -Destination "$MyCurrentPath\current__Build.sql" -Force
                }
                else
                {$problems += "SQLite couldn't create the build script for $($param1.Version) $($param1.RDBMS)"}
            }
            default
                {
                $problems += "cannot do a build script for $($param1.Version) $($param1.RDBMS) "
                }
		}
		if ($problems.count -gt 0)
		{ $Param1.Problems.'CreateBuildScriptIfNecessary' += $problems; }
		else
		{
			$Param1.WriteLocations.'CreateBuildScriptIfNecessary' = "$MyDatabasePath\V$($param1.Version)__Build.sql";
		}
	}
	
	else {$Param1.feedback.'CreateBuildScriptIfNecessary'="This version '$($param1.Version)' already has a build script at $MyDatabasePath " }
	
}


<#This scriptblock executes SQL that produces a report in XML or JSON from the database
#>

$ExecuteTableSmellReport = {
	Param ($param1) # $ExecuteTableSmellReport - parameter is a hashtable 
	$problems = @()
	@('server', 'database', 'version', 'project') | foreach{
		if ($param1.$_ -eq $null)
		{ write-error "no value for '$($_)'" }
	}
	<#if ($param1.EscapedProject -eq $null) #check that escapedValues are in place
	{
		$EscapedValues = $param1.GetEnumerator() |
		where { $_.Name -in ('server', 'Database', 'Project') } | foreach{
			@{ "Escaped$($_.Name)" = ($_.Value.Split([IO.Path]::GetInvalidFileNameChars()) -join '_') }
		}
		$EscapedValues | foreach{ $param1 += $_ }
	}#>
    $EscapedProject=($Param1.Project.Split([IO.Path]::GetInvalidFileNameChars()) -join '_') -ireplace '\.','-'
	$MyDatabasePath = 
        if  ($param1.directoryStructure -in ('classic',$null)) #If the $ReportDirectory has a value
          {"$($env:USERPROFILE)\$($param1.Reportdirectory)$($escapedProject)\$($param1.Version)\Reports"} 
        else {"$($param1.reportLocation)\$($param1.Version)\Reports"} #else the simple version
	if (-not (Test-Path -PathType Container $MyDatabasePath))
	{
		# does the path to the reports directory exist?
		# not there, so we create the directory 
		$null = New-Item -ItemType Directory -Force $MyDatabasePath;
	}
	$MyOutputReport = "$MyDatabasePath\TableIssues.JSON"
	#the alias must be set to the path of your installed version of SQL Compare
	$command=$null;
    $command = get-command SQLCmd -ErrorAction Ignore 
	if ($command -eq $null)
	{  if ($SQLCmdAlias -ne $null)
        {Set-Alias SQLCmd   $SQLCmdAlias  }
    else
        {$problems += 'You must have provided a path to SQLcmd.exe in the ToolLocations.ps1 file in the resources folder'}
        }
	#is that alias correct?
	$query = @'
SET NOCOUNT ON
/**
summary:   >
 This query finds the following table smells
 1/ is a Wide table (set this to what you consider to be wide)
 2/ is a Heap
 3/ is an undocumented table
 4/ Has no Primary Key
 5/ Has ANSI NULLs set to OFF
 6/ Has no index at all
 7/ No candidate key (unique constraint on column(s))
 8/ Has disabled Index(es)
 9/ has leftover fake index(es)
10/ has a column collation different from the database
11/ Has a surprisingly low Fill-Factor
12/ Has disabled constraint(s)'
13/ Has untrusted constraint(s)'
14/ Has a disabled Foreign Key'
15/ Has untrusted FK'
16/ Has unrelated to any other table'
17/ Has a deprecated LOB datatype
18/ Has unintelligible column names'
19/ Has a foreign key that has no index'
20/ Has a GUID in a clustered Index
21/ Has non-compliant column names'
22/ Has a trigger that has'nt got NOCOUNT ON'
23/ Is not referenced by any procedure, view or function'
24/ Has  a disabled trigger' 
25/ Can't be indexed'
Revisions:
 - Author: Phil Factor
   Version: 1.1
   Modifications:
	-  added tests as suggested by comments to blog
   Date: 30 Mar 2016
 - Author: Phil Factor
   Version: 1.2
   Modifications:
	-  tidying, added five more smells
   Date: 10 July 2020
 - Author: Phil Factor
   Version: 1.3
   Modifications:
	-  re-engineered it for JSON output
   Date: 26 March 2021
 
 returns:   >
 single result of table name, and list of problems        
**/
declare  @TableSmells table (object_id INT, problem VARCHAR(200)) 
INSERT INTO @TableSmells (object_id, problem)
 SELECT object_id, 'wide (more than 15 columns)' AS Problem
          FROM sys.tables /* see whether the table has more than 15 columns */
          WHERE max_column_id_used > 15
        UNION ALL
        SELECT DISTINCT sys.tables.object_id, 'heap'
          FROM sys.indexes /* see whether the table is a heap */
            INNER JOIN sys.tables
              ON sys.tables.object_id = sys.indexes.object_id
          WHERE sys.indexes.type = 0
        UNION ALL
        SELECT s.object_id, 'Undocumented table'
          FROM sys.tables AS s /* it has no extended properties */
            LEFT OUTER JOIN sys.extended_properties AS ep
              ON s.object_id = ep.major_id AND minor_id = 0
          WHERE ep.value IS NULL
        UNION ALL
        SELECT sys.tables.object_id, 'No primary key'
          FROM sys.tables /* see whether the table has a primary key */
          WHERE ObjectProperty(object_id, 'TableHasPrimaryKey') = 0
        UNION ALL
        SELECT sys.tables.object_id, 'has ANSI NULLs set to OFF'
          FROM sys.tables /* see whether the table has ansii NULLs off*/
          WHERE ObjectPropertyEx(object_id, 'IsAnsiNullsOn') = 0
       UNION ALL
        SELECT sys.tables.object_id, 'No index at all'
          FROM sys.tables /* see whether the table has any index */
          WHERE ObjectProperty(object_id, 'TableHasIndex') = 0
        UNION ALL
        SELECT sys.tables.object_id, 'No candidate key'
          FROM sys.tables /* if no unique constraint then it isn't relational */
          WHERE ObjectProperty(object_id, 'TableHasUniqueCnst') = 0
            AND ObjectProperty(object_id, 'TableHasPrimaryKey') = 0
        UNION ALL
        SELECT DISTINCT object_id, 'disabled Index(es)'
          FROM sys.indexes /* don't leave these lying around */
          WHERE is_disabled = 1
        UNION ALL
        SELECT DISTINCT object_id, 'leftover fake index(es)'
          FROM sys.indexes /* don't leave these lying around */
          WHERE is_hypothetical = 1
        UNION ALL
        SELECT c.object_id,
          'has a column ''' + c.name + ''' that has a collation '''
          + collation_name + ''' different from the database'
          FROM sys.columns AS c
          WHERE Coalesce(collation_name, '') 
		  <> DatabasePropertyEx(Db_Id(), 'Collation')
        UNION ALL
        SELECT DISTINCT object_id, 'surprisingly low Fill-Factor'
          FROM sys.indexes /* a fill factor of less than 80 raises eyebrows */
          WHERE fill_factor <> 0
            AND fill_factor < 80
            AND is_disabled = 0
            AND is_hypothetical = 0
        UNION ALL
        SELECT DISTINCT parent_object_id, 'disabled constraint(s)'
          FROM sys.check_constraints /* hmm. i wonder why */
          WHERE is_disabled = 1
        UNION ALL
        SELECT DISTINCT parent_object_id, 'untrusted constraint(s)'
          FROM sys.check_constraints /* ETL gone bad? */
          WHERE is_not_trusted = 1
        UNION ALL
        SELECT DISTINCT parent_object_id, 'disabled FK'
          FROM sys.foreign_keys /* build script gone bad? */
          WHERE is_disabled = 1
        UNION ALL
        SELECT DISTINCT parent_object_id, 'untrusted FK'
          FROM sys.foreign_keys /* Why do you have untrusted FKs?       
      Constraint was enabled without checking existing rows;
      therefore, the constraint may not hold for all rows. */
          WHERE is_not_trusted = 1
        UNION ALL
        SELECT object_id, 'unrelated to any other table'
          FROM sys.tables /* found a simpler way! */
          WHERE ObjectPropertyEx(object_id, 'TableHasForeignKey') = 0
            AND ObjectPropertyEx(object_id, 'TableHasForeignRef') = 0
        UNION ALL
        SELECT object_id, 'deprecated LOB datatype'
          FROM sys.tables /* found a simpler way! */
          WHERE ObjectPropertyEx(object_id, 'TableHasTextImage') = 1 
       UNION ALL
        SELECT DISTINCT object_id, 'unintelligible column names'
          FROM sys.columns /* column names with no letters in them */
          WHERE name COLLATE Latin1_General_CI_AI NOT LIKE '%[A-Z]%' COLLATE Latin1_General_CI_AI
        UNION ALL
        SELECT keys.parent_object_id,
          'foreign key ' + keys.name + ' that has no supporting index'
          FROM sys.foreign_keys AS keys
            INNER JOIN sys.foreign_key_columns AS TheColumns
              ON keys.object_id = constraint_object_id
            LEFT OUTER JOIN sys.index_columns AS ic
              ON ic.object_id = TheColumns.parent_object_id
             AND ic.column_id = TheColumns.parent_column_id
             AND TheColumns.constraint_column_id = ic.key_ordinal
          WHERE ic.object_id IS NULL
        UNION ALL
        SELECT Ic.object_id, Col_Name(Ic.object_id, Ic.column_id)
          + ' is a GUID in a clustered index' /* GUID in a clustered IX */
          FROM sys.index_columns AS Ic
			INNER JOIN sys.tables AS tables
			ON tables.object_id = Ic.object_id
            INNER JOIN sys.columns AS c
              ON c.object_id = Ic.object_id AND c.column_id = Ic.column_id
            INNER JOIN sys.types AS t
              ON t.system_type_id = c.system_type_id
            INNER JOIN sys.indexes AS i
              ON i.object_id = Ic.object_id AND i.index_id = Ic.index_id
          WHERE t.name = 'uniqueidentifier'
            AND i.type_desc = 'CLUSTERED'
        UNION ALL
        SELECT DISTINCT object_id, 'non-compliant column names'
          FROM sys.columns /* column names that need delimiters*/
          WHERE name COLLATE Latin1_General_CI_AI LIKE '%[^_@$#A-Z0-9]%' COLLATE Latin1_General_CI_AI
        UNION ALL /* Triggers lacking `SET NOCOUNT ON`, which can cause unexpected results WHEN USING OUTPUT */
        SELECT ta.object_id,
          'This table''s trigger, ' + Object_Name(tr.object_id)
          + ', has''nt got NOCOUNT ON'
          FROM sys.tables AS ta /* see whether the table has any index */
            INNER JOIN sys.triggers AS tr
              ON tr.parent_id = ta.object_id
            INNER JOIN sys.sql_modules AS mo
              ON tr.object_id = mo.object_id
          WHERE definition NOT LIKE '%set nocount on%'
        UNION ALL /* table not referenced by any routine */
        SELECT sys.tables.object_id,
          'not referenced by procedure, view or function'
          FROM sys.tables /* found a simpler way! */
            LEFT OUTER JOIN sys.sql_expression_dependencies
              ON referenced_id = sys.tables.object_id
          WHERE referenced_id IS NULL
        UNION ALL
        SELECT DISTINCT parent_id, 'has a disabled trigger'
          FROM sys.triggers
          WHERE is_disabled = 1 AND parent_id > 0
        UNION ALL
        SELECT sys.tables.object_id, 'can''t be indexed'
          FROM sys.tables /* see whether the table has a primary key */
          WHERE ObjectProperty(object_id, 'IsIndexable') = 0
DECLARE @json NVARCHAR(MAX)
SELECT @json = (SELECT TableName, problem from
	(SELECT DISTINCT  Object_Schema_Name(Object_ID) + '.' + Object_Name(Object_ID) AS TableName,object_id, Count(*) AS smells
FROM @TableSmells GROUP BY Object_ID)f(TableName,Object_id, Smells)
INNER JOIN @TableSmells AS problems ON f.object_id=problems.object_id
ORDER BY smells desc
FOR JSON AUTO)
SELECT @Json
'@
	
	if (!([string]::IsNullOrEmpty($param1.uid)) -and ([string]::IsNullOrEmpty($param1.pwd)))
	{ $problems += 'No password is specified' }
	If (!(Test-Path -PathType Leaf  $MyOutputReport) -and ($problems.Count -eq 0))
	{
		if (!([string]::IsNullOrEmpty($param1.uid)))
		{
			$MyJSON = sqlcmd -S "$($param1.server)" -d "$($param1.database)" `
							 -Q `"$query`" -U $($param1.uid) -P $($param1.pwd) -o $MyOutputReport -u -y0
			$arguments = "$($param1.server) -d $($param1.database) -U $($param1.uid) -P $($param1.pwd) -o $MyOutputReport"
		}
		else
		{
			$MyJSON = sqlcmd -S "$($param1.server)" -d "$($param1.database)" -Q `"$query`" -E -o $MyOutputReport -u -y0
			$arguments = "$($param1.server) -d $($param1.database) -o $MyOutputReport"
		}
		if (!($?))
		{
			#report a problem and send back the args for diagnosis (hint, only for script development)
			$Problems += "sqlcmd failed with code $LASTEXITCODE, $Myversions, with parameters $arguments"
		}
		$possibleError = Get-Content -Path $MyOutputReport -raw
		if ($PossibleError -like '*Sqlcmd: Error*')
		{
			$Problems += $possibleError;
			Remove-Item $MyOutputReport;
		}
		
	}
	if ($problems.Count -gt 0)
	{
		$Param1.Problems.'ExecuteTableSmellReport' += $problems;
	}
	else
	{
		$Param1.WriteLocations.'ExecuteTableSmellReport' = $MyOutputReport;
	}
}

<# This places in a report a json report of the documentation of every table and its
columns. If you add or change tables, this can be subsequently used to update the 
AfterMigrate callback script $param1=$dbDetails
for the documentation */#>

$ExecuteTableDocumentationReport = {
	Param ($param1) # $ExecuteTableDocumentationReport  - parameter is a hashtable
	
	$PSDefaultParameterValues['Out-File:Encoding'] = 'utf8'
	$problems = @()
	@('server', 'database', 'version', 'project', 'rdbms') | foreach{
		$value = "$($param1.$_.Trim())"
		if ([string]::IsNullOrEmpty($value))
		{ $Problems = "no value for '$($_)'" }
	}
	$escapedProject = ($Param1.project.Split([IO.Path]::GetInvalidFileNameChars()) -join '_') -ireplace '\.', '-'
	$MyDatabasePath =
	if ($param1.directoryStructure -in ('classic', $null)) #If the $ReportDirectory has a value
	{ "$($env:USERPROFILE)\$($param1.Reportdirectory)$($escapedProject)\$($param1.Version)\Reports" }
	else { "$($param1.reportLocation)\$($param1.Version)\Reports" } #else the simple version
	if (-not (Test-Path -PathType Container $MyDatabasePath))
	{
		# does the path to the reports directory exist?
		# not there, so we create the directory 
		$null = New-Item -ItemType Directory -Force $MyDatabasePath;
	}
	$MyOutputReport = "$MyDatabasePath\TableDocumentation.JSON"
	#handy stuff for where clauses
	$ListOfSchemas = ($param1.schemas -split ',' | foreach{ "'$_'" }) -join ',';
	$FlywayTableName = ($param1.flywayTable -split '\.')[1]
	
	switch -Regex ($param1.RDBMS)
	{
		'mysql|mariadb'   {
			$ColumnComments = @"
SELECT 
  CONCAT( c.table_schema, '.',c.TABLE_NAME) AS TableObjectName, 
  case when v.Table_Name IS NULL then 'user table' ELSE 'view' END AS "Type", 
  COLUMN_NAME AS "The_Column_Name", COLUMN_COMMENT as "Description" 
FROM information_schema.columns  c
LEFT OUTER JOIN information_schema.views v 
ON c.TABLE_NAME=v.Table_Name
AND v.table_Schema=c.table_Schema
WHERE c.table_schema NOT IN ('information_schema','mysql','performance_schema','sys')
	and c.TABLE_NAME <> 'flyway_schema_history'
ORDER BY c.TABLE_NAME, ordinal_Position;
"@;
			#Although there is metadata storage for comments in views, it isnt used.
			$ObjectComments =@"
SELECT CONCAT( table_schema, '.',TABLE_NAME) AS TableObjectName,
'user table' as "Type",
TABLE_COMMENT AS "Description" 
FROM information_schema.tables
WHERE table_schema NOT IN ('information_schema','mysql','performance_schema','sys')
	AND TABLE_NAME <> '$FlywayTableName'
	AND TABLE_TYPE = 'BASE TABLE'
		
"@
		}
		
		'postgresql' {
			#fetch all the relations (anything that produces columns)
			$ColumnComments =  @"
            SELECT json_agg(e) 
            FROM (
            SELECT isc.table_schema||'.'||isc.table_name AS TableObjectName,
	CASE WHEN views.table_name IS NULL THEN 'table' else 'view' END AS TYPE,
	COLUMN_NAME AS "The_Column_Name",
   -- obj_description(format('%s.%s',isc.table_schema,isc.table_name)::regclass::oid, 'pg_class') as table_description,
    pg_catalog.col_description(format('%s.%s',isc.table_schema,isc.table_name)::regclass::oid,isc.ordinal_position) as Description
FROM
   information_schema.columns isc
 left outer JOIN (SELECT  table_schema,table_name FROM information_schema."views")as views
            ON isc.table_schema=views.table_schema AND  isc.TABLE_NAME=views.table_name
            WHERE isc.table_catalog=current_database() AND isc.table_schema NOT IN ('pg_catalog','information_schema')
            and isc.TABLE_NAME <> '$FlywayTableName'
            ORDER BY isc.table_schema, isc.TABLE_NAME, isc.ordinal_position        
                ) e;
"@
			$ObjectComments =@"
SELECT json_agg(e) 
  FROM (
    SELECT isc.table_schema||'.'||isc.table_name AS TableObjectName,
	replace(lower(table_type),'base ','') AS TYPE,
	obj_description(format('%s.%s',isc.table_schema,isc.table_name)::regclass::oid, 'pg_class') as Description
     FROM
      information_schema.tables isc
      WHERE isc.table_catalog=current_database() AND isc.table_schema NOT IN ('pg_catalog','information_schema')
      and isc.TABLE_NAME <> '$FlywayTableName'
     ORDER BY isc.table_schema, isc.TABLE_NAME  
    ) e;
"@
		}
		'Sqlserver'   {
			# start of SQL Server comments/description
			$ColumnComments = @"
SELECT Object_Schema_Name (objects.object_id) + '.' + objects.name AS "TableObjectName",
       Lower (Replace (objects.type_desc, '_', ' ')) AS "Type",
       columns.name AS "The_Column_Name",
       Convert (VARCHAR(MAX), epcolumn.value) AS "Description"
  FROM
  sys.columns
    INNER JOIN sys.objects
      ON objects.object_id = COLUMNS.object_id
    LEFT OUTER JOIN sys.extended_properties epcolumn --get any description
      ON epcolumn.major_id = objects.object_id
     AND epcolumn.minor_id = COLUMNS.column_id
     AND epcolumn.class = 1
     AND epcolumn.name = 'MS_Description' --you may choose a different name
  WHERE
  columns.object_id = objects.object_id AND is_ms_shipped = 0
  AND Object_Schema_Name (objects.object_id) IN ($ListOfSchemas) 
  AND objects.name <> '$FlywayTableName'
  order by objects.object_id, columns.column_id
  FOR JSON auto, INCLUDE_NULL_VALUES
"@
			$ObjectComments = @"
SELECT Object_Schema_Name (tables.object_id) + '.' + tables.name AS "TableObjectName",
       Lower (Replace (tables.type_desc, '_', ' ')) AS "Type",
       Convert (VARCHAR(MAX), ep.value) AS "Description"
  FROM
  sys.objects tables
    LEFT OUTER JOIN sys.extended_properties ep
      ON ep.major_id = tables.object_id
     AND ep.minor_id = 0
     AND ep.name = 'MS_Description'
  WHERE
  tables.type IN ('IF', 'FT', 'TF', 'U', 'V')
  AND Object_Schema_Name (tables.object_id) IN ($ListOfSchemas) 
  AND Tables.name <> '$FlywayTableName'
  FOR JSON auto, INCLUDE_NULL_VALUES
"@
		}
	}
	if ($problems.Count -eq 0)
	{
		$Columns = Execute-SQL $param1 $ColumnComments | convertFrom-JSON
		$Objects = Execute-SQL $param1 $ObjectComments | convertFrom-JSON
		#Create the JSON documentation file
		$objects | foreach{
			$What = $_;
			$TableObjectName = $_.TableObjectName;
			$Type = $_.Type;
			$TheColumns = [ordered]@{ }
			$Columns | where { $_.TableObjectName -eq $TableObjectName -and $_.Type -eq $Type } | foreach{
				$TheColumns."$($_.The_Column_Name)" = "$($_.Description)";
			}
			[ordered]@{
				"TableObjectName" = $TableObjectName;
				"Type" = $Type;
				"Description" = "$($_.Description)";
				"TheColumns" = $TheColumns
				
			}
			
		} | convertTo-json > $MyOutputReport
		
	}
	
	if ($problems.Count -gt 0)
	{
		$Param1.Problems.'ExecuteTableDocumentationReport' += $problems;
	}
	else
	{
		$Param1.WriteLocations.'ExecuteTableDocumentationReport' = $MyOutputReport;
	}
}


<#

$SaveDatabaseModelIfNecessary


This writes a JSON model of the database to a file that can be used subsequently
to check for database version-drift or to create a narrative of changes for the
flyway project between versions.
To run this, you need to provide values for 
'server', The name of the database server
'database', The name of the database
'version', The version directory that is to be ascribed to the file
'project', The name of the whole project for the output filenames
'RDBMS', the rdbms being used, e.g. sqlserver, mysql, mariadb, postgresql, sqlite
'schemas', the schemas to be used to create the model
'flywayTable' the name and schema of the flyway table 
$param1=$DBDetails*/#>
$SaveDatabaseModelIfNecessary = {
	Param ($param1,
		$MyOutputReport = $null,
		$MyCurrentReport = $null,
		$MyModelPath = $null) # $SaveDatabaseModelIfNecessary - dont delete this
	$PSDefaultParameterValues['Out-File:Encoding'] = 'utf8' #we'll be using out redirection
	$problems = @() #none yet!
	$feedback = @();
    $AlreadyDone =$false;
	#check that you have the  entries that we need in the parameter table.
	if ($MyOutputReport -eq $null -and $MyCurrentReport -eq $null -and $MyModelPath -eq $null)
	{
		$Essentials = @('server', 'database', 'version', 'project', 'RDBMS', 'schemas', 'flywayTable')
	}
	else #slightly less required for an ad-hoc model.
	{
		$Essentials = @('server', 'database', 'RDBMS', 'schemas', 'flywayTable')
	}
	$Essentials | foreach{
		if ([string]::IsNullOrEmpty($param1.$_))
		{ $Problems += "no value for '$($_)'" }
	}
	if ($MyOutputReport -eq $null -or $MycurrentReport -eq $null -or $MyModelPath -eq $null)
	#if all parameters not provided
	{
		$escapedProject = ($Param1.project.Split([IO.Path]::GetInvalidFileNameChars()) -join '_') -ireplace '\.', '-'
		if ($param1.directoryStructure -in ('classic', $null)) #If the $ReportDirectory has a classic or NULL value
		{ $where = "$($env:USERPROFILE)\$($param1.Reportdirectory)$($escapedProject)" }
		else { $where = "$($param1.reportLocation)" }
		$MyDatabasePath = "$where\$($param1.Version)\Reports";
		$MyCurrentPath = "$where\current";
		if ($MyModelPath -eq $null) { $MyModelPath = "$where\$($param1.Version)\model" };
		#the object-level models
	}
	#else the simple version
	# so if not specified in the parameters, generate the correct location.
	if ($MyOutputReport -eq $null)
	{ $MyOutputReport = "$MyDatabasePath\DatabaseModel.JSON" }
	if ($MyCurrentReport -eq $null)
	{ $MyCurrentReport = "$MyCurrentPath\Reports\DatabaseModel.JSON" }
	#handy stuff for where clauses
	$ListOfSchemas = ($param1.schemas -split ',' | foreach{ "'$_'" }) -join ',';
	if ($param1.flywayTable -ne $null)
	{ $FlywayTableName = ($param1.flywayTable -split '\.')[1] }
	else
	{ $FlywayTableName = 'flyway_schema_history' }
	if (!(Test-Path -PathType Leaf $MyOutputReport)) #only do it once
	{
		try
		{
			@($MyDatabasePath, "$MyCurrentPath\Reports") | foreach {
				if (Test-Path -PathType Leaf $_)
				{
					# does the path to the reports directory exist as a file for some reason?
					# there, so we delete it 
					remove-Item $_;
				}
				if (-not (Test-Path -PathType Container $_))
				{
					# does the path to the reports directory exist?
					# not there, so we create the directory 
					$null = New-Item -ItemType Directory -Force $_;
				}
			}
			switch -Regex ($param1.RDBMS)
			{
<# this is the section that creates a SQLite Database Model  #>
				'sqlite' {
					$SQLlines = @() #we build the SQL dynamically - SQLite is a bit awkward for metadata!
					$TablesAndViews = Execute-SQL $param1 "SELECT Name, Type FROM sqlite_schema WHERE TYPE <> 'index';" | convertfrom-json
            <# we process tables and views first but not indexes as they are actually child objects of tables  #>
					$TablesAndViews | foreach{
						$type = $_.type;
						$SQLlines += "SELECT '$($_.type)' as type, '$($_.name)' as object, Name||' '||TYPE||CASE `"notnull`" when 1 THEN ' NOT' ELSE '' END
                 ||' NULL '||CASE WHEN dflt_value IS NOT NULL THEN 'DEFAULT ('||`"dflt_value`"||')' ELSE '' end as col, pk
                  FROM pragma_table_info('$($_.name)')"
					}
            <# we want to scoop up as much metadata as we can in one query so we do this with a UNION ALL. #>
					$query = ($SQLlines -join "`r`nUNION ALL`r`n") + ';'
            <# The query was created dynamically, now we execute it  #>
					$TheRelationMetadata = Execute-SQL $param1 $query | ConvertFrom-json
					
            <# Now get the details of all the indexes that aren't primary keys, including the columns,  #>
					$indexes = Execute-SQL $param1 @"
            SELECT m.name as index_name, m.tbl_Name as table_name, i.seqno, i.cid, i.name as column_name
              FROM sqlite_schema AS m,
              pragma_index_info(m.name) AS i
               WHERE TYPE='index' AND sql IS NOT NULL;
"@ | ConvertFrom-Json
					
            <# we get the list of different base types (obvious in SQLite but it can get tricky with other
            RDBMS  #>
					$THeTypes = $TablesAndViews | Select type -Unique #|foreach{$_.type}
            <# OK. we now have to assemble all this into a model that is as human-friendly as possible  #>
					$SchemaTree = @{ } <# This will become our model of the schema. Fist we put in
            all the types of relations  #>
					$TheTypes | foreach{
						$SchemaTree | add-member -NotePropertyName $_.type -NotePropertyValue @{ }
					}
					
            <# now inject all the objects into the schema tree. First we get all the relations  #>
					$TheRelationMetadata | Select type, object -Unique | foreach{
						$type = $_.type;
						$object = $_.object;
						$pk = @{ }
						$TheColumnList = $TheRelationMetadata |
						where { $_.type -eq $type -and $_.object -eq $object } -OutVariable pk |
						foreach{ $_.col }
						$primaryKey = @();
						$SchemaTree.$type += @{ $object = @{ 'columns' = $TheColumnList } }
						$primaryKey = $pk | Where { $_.pk -gt 0 } |
						Sort-Object -Property pk |
						Foreach{ [regex]::matches($_.col, '\A\S{1,80}').value }
						if ($primaryKey.count -gt 0)
						{
							$SchemaTree.$type.$object += @{ 'PrimaryKey' = $primaryKey }
						}
					}
            <# now stitch in the indexes with their columns  #>
					$indexes | Select table_name, index_name -Unique | foreach{
						$indexedTable = $_.table_name
						$indexName = $_.index_name
						$columns = $indexes |
						where{ $_.table_name -eq $indexedTable -and $_.index_name -eq $indexName } |
						Sort-Object -Property seqno | Select -ExpandProperty column_name
						$SchemaTree.table.$indexedTable.indexes += @{ $indexName = $columns }
					}
					$SchemaTree | convertTo-json -depth 10 > "$MyOutputReport"
					$SchemaTree | convertTo-json -depth 10 > "$MycurrentReport"
					
				} #end of SQLite version
<# this is the section that creates a PostgreSQL Database Model based where
possible on information schema #>				
				'postgresql' {
					#fetch all the relations (anything that produces columns)
					$query = @"
            SELECT json_agg(e) 
            FROM (
            Select columns.table_schema as schema, columns.TABLE_NAME as object, COLUMN_NAME||' '||data_type||
               CASE WHEN data_type LIKE 'char%' THEN ' ('||character_maximum_length||')'ELSE''END||
	            CASE WHEN numeric_precision IS NOT NULL THEN 
		            CASE WHEN data_type LIKE '%int%' THEN ' '
		            ELSE ' ('|| numeric_precision_radix ||','|| numeric_scale || ')' END
	            ELSE '' END||
	            CASE is_identity WHEN 'YES' THEN 'GENERATED '||identity_generation||' AS IDENTITY'
	            ELSE	 
		            CASE is_nullable WHEN 'NO' THEN ' NOT' ELSE '' END ||
	            ' NULL ' ||
		            CASE when column_default is NOT NULL THEN 'DEFAULT ('||
		              column_default || ')' ELSE '' END
	            END AS COLUMN,
	            CASE WHEN views.table_name IS NULL THEN 'table' else 'view' END AS type
            FROM information_schema."columns" 
            left outer JOIN (SELECT  table_schema,table_name FROM information_schema."views")as views
            ON columns.table_schema=views.table_schema AND  columns.TABLE_NAME=views.table_name
            WHERE columns.table_catalog=current_database() AND columns.table_schema NOT IN ('pg_catalog','information_schema')
            and columns.TABLE_NAME <> '$FlywayTableName'
            ORDER BY columns.table_schema, columns.TABLE_NAME, ordinal_position
                ) e;
"@
					$TheRelationMetadata = Execute-SQL $param1 $query | ConvertFrom-json
					#now get the details of the routines
					$query = @"
 SELECT json_agg(e) 
            FROM (SELECT 
		Routine_name AS "name", Routine_Schema AS "schema", 
		CONCAT(Routine_Schema,'.', Routine_name) AS "fullname", 
		LOWER(routine_type) AS "type",
		routine_definition AS definition, 
		MD5(routine_definition) AS "hash",
		description AS "comment"
   FROM information_schema.routines
	INNER JOIN pg_proc ON proname LIKE routine_name
	LEFT OUTER JOIN pg_description
	ON OID= objoid
	WHERE Routine_Schema IN ($ListOfSchemas) 	
UNION ALL
SELECT 
		Table_name AS "name", 
		Table_Schema AS "schema", 
		CONCAT(table_Schema,'.', table_name) AS "fullname", 
		LOWER(replace(table_type,'BASE ','')) AS "type",
		'' AS definition, 
		MD5('') AS HASH,
		description AS COMMENT
	FROM information_schema.tables
		INNER JOIN pg_class ON relname LIKE table_name
	LEFT OUTER JOIN pg_description
	ON OID= objoid
	WHERE Table_Schema IN ($listOfSchemas) 
		AND TABLE_NAME NOT LIKE '$FlywayTableName'
                ) e;
"@
					$Routines = Execute-SQL $param1 $query | ConvertFrom-json
					#now do the constraints
					$query = @"
            SELECT json_agg(e) 
            FROM (SELECT lower(tc.constraint_type) as type, tc.table_schema as schema, 
               tc.Table_name, kcu.constraint_name, kcu.column_name, 
					ordinal_position,
					rel_tco.table_schema || '.' || rel_tco.table_name AS "referenced_table",
					kcu.column_name AS "referenced_column_name"
            FROM information_schema.table_constraints AS tc 
            JOIN information_schema.key_column_usage AS kcu 
              ON tc.constraint_name = kcu.constraint_name 
              AND tc.table_name=kcu.table_name
              AND tc.table_schema=kcu.table_schema
left outer join information_schema.referential_constraints rco
          on tc.constraint_schema = rco.constraint_schema
          and tc.constraint_name = rco.constraint_name
left outer join information_schema.table_constraints rel_tco
          on rco.unique_constraint_schema = rel_tco.constraint_schema
          and rco.unique_constraint_name = rel_tco.constraint_name
           WHERE tc.table_catalog=current_database() 
              and tc.Table_name <> '$FlywayTableName' 
              AND tc.table_schema NOT IN ('pg_catalog','information_schema')
                ) e;
"@
					$Constraints = Execute-SQL $param1 $query | ConvertFrom-json
            <# Now get the details of all the indexes that aren't primary keys, including the columns,  #>
					$indexes = Execute-SQL $param1 @"
            SELECT json_agg(e) 
            FROM(SELECT distinct
	             allindexes.schemaname AS schema,
                t.relname as table_name,
                i.relname as index_name,
                a.attname AS "column_name",
                allindexes.indexdef AS definition
            from
                pg_index ix 
	             INNER join pg_class t on t.oid = ix.indrelid
                INNER JOIN  pg_class i ON i.oid = ix.indexrelid
                inner join pg_attribute a on a.attrelid = t.oid
                LEFT OUTER JOIN pg_indexes allindexes ON t.relname= allindexes.tablename AND i.relname=allindexes.indexname
            where
                a.attnum = ANY(ix.indkey)
                and t.relkind = 'r'
                AND indisprimary=FALSE AND indisunique=FALSE
                AND allindexes.schemaname <>'pg_catalog'
                AND t.relname <> '$FlywayTableName'
            order by
                t.relname,
                i.relname
                ) e;    
"@ | ConvertFrom-Json
					
					#now get all the triggers
					$triggers = Execute-SQL $param1 @'
            SELECT json_agg(e) 
            FROM(
            SELECT TRIGGER_SCHEMA as schema, TRIGGER_NAME, event_object_schema, event_object_table
            FROM information_schema.triggers t
            WHERE t.trigger_catalog=current_database() 
              AND t.trigger_schema NOT IN ('pg_catalog','information_schema')
                ) e; 
'@ | ConvertFrom-Json
					
            <# RDBMS  #>
					$THeTypes = $Routines | Select schema, type -Unique
            <# OK. we now have to assemble all this into a model that is as human-friendly as possible  #>
					$SchemaTree = @{ } <# This will become our model of the schema. Fist we put in
            all the types of relations  #>
					
					
					$TheTypes | Select -ExpandProperty schema -Unique | foreach{
						$TheSchema = $_;
						$ourtypes = @{ }
						$TheTypes | where { $_.schema -eq $TheSchema } | Select -ExpandProperty type | foreach{ $OurTypes += @{ $_ = @{ } } }
						$SchemaTree | add-member -NotePropertyName $TheSchema -NotePropertyValue $OurTypes
						
					}
					
            <# now inject all the objects into the schema tree. First we get all the relations  #>
					$TheRelationMetadata | Select schema, type, object -Unique | foreach{
						$schema = $_.schema;
						$type = $_.type;
						$object = $_.object;
						$TheColumnList = $TheRelationMetadata |
						where { $_.schema -eq $schema -and $_.type -eq $type -and $_.object -eq $object } -OutVariable pk |
						foreach{ $_.column }
						$SchemaTree.$schema.$type += @{ $object = @{ 'columns' = $TheColumnList } }
					}
					#display-object $schemaTree
            <# now stitch in the constraints with their columns  #>
					$constraints | Select schema, table_name, Type, constraint_name, referenced_table -Unique | foreach{
						$constraintSchema = $_.schema;
						$constrainedTable = $_.table_name;
						$constraintName = $_.constraint_name;
						$ConstraintType = $_.type;
						$referenced_table = $_.referenced_table;
						# get the original object
						$OriginalConstraint = $constraints |
						where{
							$_.schema -eq $constraintSchema -and
							$_.table_name -eq $constrainedTable -and
							$_.Type -eq $ConstraintType -and
							$_.constraint_name -eq $constraintName
						} | Select -first 1
						$Columns = $OriginalConstraint | Sort-Object -Property ordinal_position |
						Select -ExpandProperty column_name
						if ($ConstraintType -eq 'foreign key')
						{
							$Referencing = $OriginalConstraint | Sort-Object -Property ordinal_position |
							Select -ExpandProperty referenced_column_name
							$SchemaTree.$constraintSchema.table.$constrainedTable.$ConstraintType += @{
								$constraintName = @{ 'Cols' = $columns; 'Foreign Table' = $referenced_table; 'Referencing' = "$Referencing" }
							}
						}
						else
						{ $SchemaTree.$constraintSchema.table.$constrainedTable.$ConstraintType += @{ $constraintName = $columns } }
						
					}
              <# now stitch in the constraints with their columns  #>
					$routines | Foreach {
						$TheSchema = $_.schema;
						$TheName = $_.name;
						$TheType = $_.type;
						$TheHash = $_.hash;
						$Thecomment = $_.comment;
						$TheDefinition = $_.definition;
						$Contents = @{ }
						if (!([string]::IsNullOrEmpty($TheDefinition))) { $Contents.'definition' = $TheDefinition }
						if ($TheType -ne 'table') { $Contents.'hash' = $Thehash }
						if (!([string]::IsNullOrEmpty($Thecomment))) { $Contents.'comment' = $TheComment }
						if ($SchemaTree.$TheSchema.$TheType.$TheName -eq $null)
						{ $SchemaTree.$TheSchema.$TheType.$TheName = $Contents }
						else
						{ $SchemaTree.$TheSchema.$TheType.$TheName += $Contents }
						
					}
					
<# now stitch in the indexes with their columns  #>
					$indexes | Select schema, table_name, Type, index_name, definition -Unique | foreach{
						$indexSchema = $_.schema;
						$indexedTable = $_.table_name;
						$indexName = $_.index_name;
						$definition = $_.definition;
						$columns = $indexes |
						where{
							$_.schema -eq $indexSchema -and
							$_.table_name -eq $indexedTable -and
							$_.index_name -eq $indexName
						} |
						Select -ExpandProperty column_name
						$SchemaTree.$indexSchema.table.$indexedTable.index += @{ $indexName = @{ 'Indexing' = $columns; 'def' = "$definition" } }
					}
					
					$SchemaTree | convertTo-json -depth 10 > "$MyOutputReport"
					$SchemaTree | convertTo-json -depth 10 > "$MycurrentReport"
				}
<# this is the section that creates a MariaDB or MySQL Database Model based where
possible on information schema $param1=$dbdetails
 #>
				'mysql|mariaDB' {
					#create a delimited list for SQL's IN command
					#fetch all the relations (anything that produces columns)
					$query = @"
        SELECT c.TABLE_SCHEMA as "schema", c.TABLE_NAME as "object", 
            case when v.Table_Name IS NULL then 'Table' ELSE 'View' END AS "Type",
            c.COLUMN_NAME as "column", c.ordinal_position,
            CONCAT (column_type, 
			case when IS_nullable = 'NO' then ' NOT' ELSE '' END ,
			' NULL', 
		  	case when COLUMN_DEFAULT IS NULL then '' ELSE CONCAT (' DEFAULT (',COLUMN_DEFAULT,')') END,
			' ',
			Extra,
			' ',
			-- case COLUMN_KEY when 'PRI' then ' PRIMARY KEY' ELSE '' end,
			case when column_comment <> '' then CONCAT('-- ',column_comment) ELSE '' end
		 ) AS coltype  
        FROM information_schema.columns c 
        LEFT OUTER JOIN information_schema.views v 
        ON c.TABLE_NAME=v.Table_Name
        AND v.table_Schema=c.table_Schema
        WHERE c.table_schema in ($ListOfSchemas)
        and c.TABLE_NAME <> '$FlywayTableName';
"@
					$TheRelationMetadata = Execute-SQL $param1 $query | ConvertFrom-json
					#now get the details of the routines
					$query = @"
            SELECT 
		Routine_name AS "name", Routine_Schema AS "schema", 
		CONCAT(Routine_Schema,'.', Routine_name) AS "fullname", 
		LOWER(routine_type) AS "type",
		CONCAT(LEFT(routine_definition,800), CASE WHEN LENGTH(routine_definition)>800 THEN '...' ELSE '' END) AS definition, 
		MD5(routine_definition) AS "hash",
		Routine_Comment AS "comment"
	FROM information_schema.routines
	WHERE Routine_Schema IN ($ListOfSchemas) 
UNION ALL
SELECT 
		Table_name AS "name", 
		Table_Schema AS "schema", 
		CONCAT(table_Schema,'.', table_name) AS "fullname", 
		'table' AS "type",
		'' AS definition, 
		MD5('') AS HASH,
		TABLE_Comment AS COMMENT
	FROM information_schema.tables
	WHERE Table_type='base table' 
		AND Table_Schema IN ($ListOfSchemas) 
		AND TABLE_NAME NOT LIKE '$FlywayTableName' 
UNION ALL
	SELECT 
		Table_name AS "name", 
		Table_Schema AS "schema", 
		CONCAT(Table_Schema,'.', Table_name) AS "fullname", 
		'view' AS "type",
		View_definition AS definition, 
		MD5(View_definition) AS HASH,
		'' AS COMMENT
	FROM information_schema.views
		WHERE Table_Schema IN ($ListOfSchemas) 
		AND TABLE_NAME NOT LIKE '$FlywayTableName'
"@
					$Routines = Execute-SQL $param1 $query | ConvertFrom-json
					#now do the constraints
					$query = @"
             SELECT lower(tc.constraint_type) AS "type", tc.table_schema AS "schema", 
               tc.Table_name, kcu.constraint_name, kcu.column_name,ordinal_position,
			   concat(Referenced_table_Schema,'.',Referenced_table_name) AS "referenced_table",referenced_column_name
            FROM information_schema.table_constraints AS tc 
            JOIN information_schema.key_column_usage AS kcu 
              ON tc.constraint_name = kcu.constraint_name 
              AND tc.table_name=kcu.table_name
              AND tc.table_schema=kcu.table_schema
            WHERE tc.table_schema IN ($ListOfSchemas)
            and tc.TABLE_NAME <> '$FlywayTableName'; 
"@
					$Constraints = Execute-SQL $param1 $query | ConvertFrom-json
            <# Now get the details of all the indexes that aren't primary keys, including the columns,  #>
					$indexes = Execute-SQL $param1 @"
            SELECT Table_Schema as "schema",TABLE_NAME, Index_name, COLUMN_NAME, Seq_in_Index AS "sequence" 
            FROM information_schema.statistics
            WHERE table_Schema IN ($ListOfSchemas) 
              AND index_name NOT IN ('PRIMARY','UNIQUE')
              and TABLE_NAME <> '$FlywayTableName';    
"@ | ConvertFrom-Json
					
					#now get all the triggers
					$triggers = Execute-SQL $param1 @"
            SELECT TRIGGER_SCHEMA, TRIGGER_NAME, event_object_schema, event_object_table
            FROM information_schema.triggers t
            WHERE t.trigger_catalog IN ($ListOfSchemas); 
"@ | ConvertFrom-Json
					
            <# RDBMS  #>
					$THeTypes = $Routines | Select schema, type -Unique
            <# OK. we now have to assemble all this into a model that is as human-friendly as possible  #>
					$SchemaTree = @{ } <# This will become our model of the schema. Fist we put in
            all the types of relations  #>
					
					
					$TheTypes | Select -ExpandProperty schema -Unique | foreach{
						$TheSchema = $_;
						$ourtypes = @{ }
						$TheTypes | where { $_.schema -eq $TheSchema } | Select -ExpandProperty type | foreach{ $OurTypes += @{ $_ = @{ } } }
						$SchemaTree | add-member -NotePropertyName $TheSchema -NotePropertyValue $OurTypes
						
					}
					
            <# now inject all the objects into the schema tree. First we get all the relations  #>
					$TheRelationMetadata | Select schema, type, object -Unique | foreach{
						$schema = $_.schema;
						$type = $_.type;
						$object = $_.object;
						$TheColumnList = $TheRelationMetadata |
						where { $_.schema -eq $schema -and $_.type -eq $type -and $_.object -eq $object } -OutVariable pk |
						Sort-Object -Property ordinal_position |
						foreach{ "$($_.column) $($_.coltype)" }
						$SchemaTree.$schema.$type += @{ $object = @{ 'columns' = $TheColumnList } }
					}
					
            <# now stitch in the constraints with their columns  #>
					$constraints | Select schema, table_name, Type, constraint_name, referenced_table -Unique | foreach{
						$constraintSchema = $_.schema;
						$constrainedTable = $_.table_name;
						$constraintName = $_.constraint_name;
						$ConstraintType = $_.type;
						$referenced_table = $_.referenced_table;
						# get the original object
						$OriginalConstraint = $constraints |
						where{
							$_.schema -eq $constraintSchema -and
							$_.table_name -eq $constrainedTable -and
							$_.Type -eq $ConstraintType -and
							$_.constraint_name -eq $constraintName
						} | Select -first 1
						$Columns = $OriginalConstraint | Sort-Object -Property ordinal_position |
						Select -ExpandProperty column_name
						if ($ConstraintType -eq 'foreign key')
						{
							$Referencing = $OriginalConstraint | Sort-Object -Property ordinal_position |
							Select -ExpandProperty referenced_column_name
							$SchemaTree.$constraintSchema.table.$constrainedTable.$ConstraintType += @{
								$constraintName = @{ 'Cols' = $columns; 'Foreign Table' = $referenced_table; 'Referencing' = "$Referencing" }
							}
						}
						else
						{ $SchemaTree.$constraintSchema.table.$constrainedTable.$ConstraintType += @{ $constraintName = $columns } }
						
					}
					
            <# now stitch in the indexes with their columns  #>
					$indexes | Select schema, table_name, Type, index_name, definition -Unique | foreach{
						$indexSchema = $_.schema;
						$indexedTable = $_.table_name;
						$indexName = $_.index_name;
						$definition = $_.definition;
						$columns = $indexes |
						where{
							$_.schema -eq $indexSchema -and
							$_.table_name -eq $indexedTable -and
							$_.index_name -eq $indexName
						} |
						Select -ExpandProperty column_name
						$SchemaTree.$indexSchema.table.$indexedTable.index += @{ $indexName = @{ 'Indexing' = $columns; 'def' = "$definition" } }
					}
					$routines | Foreach {
						$TheSchema = $_.schema;
						$TheName = $_.name;
						$TheType = $_.type;
						$TheHash = $_.hash;
						$Thecomment = $_.comment;
						$TheDefinition = $_.definition;
						$Contents = @{ }
						if (!([string]::IsNullOrEmpty($TheDefinition))) { $Contents.'definition' = $TheDefinition }
						if ($TheType -ne 'table') { $Contents.'hash' = $Thehash }
						if (!([string]::IsNullOrEmpty($Thecomment))) { $Contents.'comment' = $TheComment }
						if ($SchemaTree.$TheSchema.$TheType.$TheName -eq $null)
						{ $SchemaTree.$TheSchema.$TheType.$TheName = $Contents }
						else
						{ $SchemaTree.$TheSchema.$TheType.$TheName += $Contents }
						
					}
					
					
					
					$SchemaTree | convertTo-json -depth 10 > "$MyOutputReport"
					$SchemaTree | convertTo-json -depth 10 > "$MycurrentReport"
				}
				
<# this is the section that creates a SQL Server Database Model based where
possible on information schema #>
				'sqlserver'  {
					#fetch all the relations (anything that produces columns) $param1=$dbDetails
					$query = @"
SELECT ParentObjects.[Schema] AS "Schema", ParentObjects.type,
       ParentObjects.Name,
       colsandparams.name + ' ' +
-- SQL Prompt formatting off
			t.[name]+ CASE --do the basic datatype
			WHEN t.[name] IN ('char', 'varchar', 'nchar', 'nvarchar')
			THEN '(' + -- we have to put in the length
				CASE WHEN ValueTypemaxlength = -1 THEN 'MAX'
				ELSE CONVERT(VARCHAR(4),
					CASE WHEN t.[name] IN ('nchar', 'nvarchar')
					THEN ValueTypemaxlength / 2 ELSE ValueTypemaxlength
					END)
				END + ')' --having to put in the length
			WHEN t.[name] IN ('decimal', 'numeric')
			--Ah. We need to put in the precision
			THEN '(' + CONVERT(VARCHAR(4), ValueTypePrecision)
					+ ',' + CONVERT(VARCHAR(4), ValueTypeScale) + ')'
			ELSE ''-- no more to do
			END+ --we've now done the datatype
			CASE WHEN XMLcollectionID <> 0 --when an XML document
			THEN --deal with object schema names
				'(' +
				CASE WHEN isXMLDocument = 1 THEN 'DOCUMENT ' ELSE 'CONTENT ' END
				+ COALESCE(
				QUOTENAME(Schemae.name) + '.' + QUOTENAME(SchemaCollection.name)
				,'NULL') + ')'
				ELSE ''
			END+Coalesce(' -- '+Description,'')	AS "Column",
			TheOrder
-- SQL Prompt formatting on
  FROM --columns, parameters, return values ColsAndParams.
    --first get all the parameters
    (SELECT cols.object_id, cols.name, 'Columns' AS "Type",
            Convert (NVARCHAR(2000), value) AS "Description",
            column_id AS TheOrder, cols.xml_collection_id,
            cols.max_length AS ValueTypemaxlength,
            cols.precision AS ValueTypePrecision,
            cols.scale AS ValueTypeScale,
            cols.xml_collection_id AS XMLcollectionID,
            cols.is_xml_document AS isXMLDocument, cols.user_type_id
       FROM
       sys.objects AS object
         INNER JOIN sys.columns AS cols
           ON cols.object_id = object.object_id
         LEFT OUTER JOIN sys.extended_properties AS EP
           ON cols.object_id = EP.major_id
          AND class = 1
          AND minor_id = cols.column_id
          AND EP.name = 'MS_Description'
       WHERE is_ms_shipped = 0
     UNION ALL
     --get in all the parameters
     SELECT params.object_id, params.name AS "Name",
            CASE WHEN parameter_id = 0 THEN 'Return' ELSE 'Parameters' END AS "Type",
            --'Parameters' AS "Type",
            Convert (NVARCHAR(2000), value) AS "Description",
            parameter_id AS TheOrder, params.xml_collection_id,
            params.max_length AS ValueTypemaxlength,
            params.precision AS ValueTypePrecision,
            params.scale AS ValueTypeScale,
            params.xml_collection_id AS XMLcollectionID,
            params.is_xml_document AS isXMLDocument, params.user_type_id
       FROM
       sys.objects AS object
         INNER JOIN sys.parameters AS params
           ON params.object_id = object.object_id
         LEFT OUTER JOIN sys.extended_properties AS EP
           ON params.object_id = EP.major_id
          AND class = 2
          AND minor_id = params.parameter_id
          AND EP.name = 'MS_Description'
       WHERE is_ms_shipped = 0) AS colsandparams
    INNER JOIN sys.types AS t
      ON colsandparams.user_type_id = t.user_type_id
    LEFT OUTER JOIN sys.xml_schema_collections AS SchemaCollection
      ON SchemaCollection.xml_collection_id = colsandparams.xml_collection_id
    LEFT OUTER JOIN sys.schemas AS Schemae
      ON SchemaCollection.schema_id = Schemae.schema_id
    RIGHT OUTER JOIN --catch parent objects without columns
      (SELECT TheObjects.object_id,
              Object_Schema_Name (TheObjects.object_id) AS "Schema",
              Replace (Lower (Replace (Replace (TheObjects.type_desc, 'user_', ''), 'sql_', '')), '_',' ') AS type, 
			  Object_Name (TheObjects.object_id) Name
         FROM sys.objects TheObjects
         WHERE
         Object_Schema_Name (TheObjects.object_id) IN ($ListOfSchemas)
     AND TheObjects.parent_object_id = 0
     AND type <> 'SQ') ParentObjects
      ON ParentObjects.object_id = colsandparams.object_id
  WHERE Object_Name (ParentObjects.object_id) <> '$FlywayTableName'
  ORDER BY
  "Schema", "type", Name, TheOrder
FOR JSON AUTO;
"@
					$TheRelationMetadata = Execute-SQL $param1 $query | ConvertFrom-json
					#now get the details of the routines
					$query = @'
    SELECT Replace (Lower (Replace(Replace(so.type_desc,'user_',''),'sql_','')), '_', ' ') as type,
        so.name, Object_Schema_Name(so.object_id) AS "schema", 
        left(definition,70)+CASE when LEN(definition)>70 THEN '...' ELSE '' END AS definition, 
        checksum(definition) AS hash 
        FROM sys.sql_modules ssm
        INNER JOIN sys.objects so
        ON so.OBJECT_ID=ssm.object_id
        FOR JSON auto               
'@
					$Routines = Execute-SQL $param1 $query | ConvertFrom-json
					
					#now do the constraints
					$query = @"
SELECT *
  FROM
    (SELECT Schema_Name (tab.schema_id) AS "schema",
            tab.name AS [table_name], 'Foreign Key' AS "type",
            fk.name AS "constraint_name", '' AS "definition",
            Schema_Name (pk_tab.schema_id) + '.' + pk_tab.name AS referenced_table,
            col.name AS column_name,
            fk_cols.constraint_column_id AS ordinal_position,
            pk_col.name AS referenced_column,
            fk_cols.constraint_column_id AS referenced_ordinal_position
       FROM
       sys.tables tab
         INNER JOIN sys.columns col
           ON col.object_id = tab.object_id
         INNER JOIN sys.foreign_key_columns fk_cols
           ON fk_cols.parent_object_id = tab.object_id
          AND fk_cols.parent_column_id = col.column_id
         INNER JOIN sys.foreign_keys fk
           ON fk.object_id = fk_cols.constraint_object_id
         INNER JOIN sys.tables pk_tab
           ON pk_tab.object_id = fk_cols.referenced_object_id
         INNER JOIN sys.columns pk_col
           ON pk_col.column_id = fk_cols.referenced_column_id
          AND pk_col.object_id = fk_cols.referenced_object_id
     UNION ALL
     SELECT Object_Schema_Name (tab.object_id) AS "Schema", tab.name,
            CASE WHEN pk.is_primary_key = 1 THEN 'primary key'
              WHEN pk.is_unique_constraint = 1 THEN 'unique key' ELSE 'index' END AS "Type",
            pk.name, '' AS "Definition", NULL AS referenced_table,
            col.name AS fk_column_name, col.column_id AS fk_ordinal_position,
            NULL AS referenced_column, NULL AS referenced_ordinal_position
       FROM
       sys.tables tab
         LEFT OUTER JOIN sys.indexes pk
           ON tab.object_id = pk.object_id
         INNER JOIN sys.index_columns ic
           ON ic.object_id = tab.object_id AND ic.index_id = pk.index_id
         INNER JOIN sys.columns col
           ON ic.object_id = col.object_id AND ic.column_id = col.column_id
     UNION ALL
     SELECT Schema_Name (t.schema_id) AS "Schema", t.[name],
            'Check constraint', con.[name] AS constraint_name,
            con.[definition], NULL, NULL, NULL, NULL, NULL
       FROM
       sys.check_constraints con
         LEFT OUTER JOIN sys.objects t
           ON con.parent_object_id = t.object_id
         LEFT OUTER JOIN sys.all_columns col
           ON con.parent_column_id = col.column_id
          AND con.parent_object_id = col.object_id
     UNION ALL
     SELECT Schema_Name (t.schema_id) AS "Schema", t.[name],
            'Check constraint', con.[name] AS constraint_name,
            con.[definition], NULL, NULL, NULL, NULL, NULL
       FROM
       sys.check_constraints con
         LEFT OUTER JOIN sys.objects t
           ON con.parent_object_id = t.object_id
         LEFT OUTER JOIN sys.all_columns col
           ON con.parent_column_id = col.column_id
          AND con.parent_object_id = col.object_id
     UNION ALL
     SELECT Schema_Name (t.schema_id) AS "Schema", t.[name],
            'Default constraint', con.[name],
            col.[name] + ' = ' + con.[definition], NULL, NULL, NULL, NULL,
            NULL
       FROM
       sys.default_constraints con
         LEFT OUTER JOIN sys.objects t
           ON con.parent_object_id = t.object_id
         LEFT OUTER JOIN sys.all_columns col
           ON con.parent_column_id = col.column_id
          AND 
           con.parent_object_id = col.object_id) f
 where f.table_name <> '$FlywayTableName'
FOR JSON AUTO
"@
					$Constraints = Execute-SQL $param1 $query | ConvertFrom-json
					if (!($constraints.Error -eq $null)) { $Problems += $constraints.Error }
            <# Now get the details of all the indexes that aren't primary keys, including the columns,  #>
					$indexes = Execute-SQL $param1 @"
    select schema_name(t.schema_id) AS "schema",
	t.name AS table_name,
	Replace (Lower (Replace(Replace(t.type_desc,'user_',''),'sql_','')), '_', ' ') as type,
        isnull(c.[name], i.[name]) as Index_name, col.name, ic.key_ordinal
    from sys.objects t
        left outer join sys.indexes i
            on t.object_id = i.object_id
        left outer join sys.key_constraints c
            on i.object_id = c.parent_object_id 
            and i.index_id = c.unique_index_id
        INNER join sys.index_columns ic
		    ON ic.object_id = t.object_id
               and ic.index_id = i.index_id
         inner join sys.columns col
               on ic.object_id = col.object_id
               and ic.column_id = col.column_id
       where is_unique = 1
       AND t.name <> '$FlywayTableName'
       AND t.is_ms_shipped <> 1
       for json path
"@ | ConvertFrom-Json
					
					#now get all the triggers
					$triggers = Execute-SQL $param1 @'
 select schema_name(tab.schema_id) AS "schema",
 tab.name as [table],
    trig.name as trigger_name,
    case when is_instead_of_trigger = 1 then 'Instead of'
        else 'After' end as [activation],
    (case when objectproperty(trig.object_id, 'ExecIsUpdateTrigger') = 1 
            then 'Update ' else '' end
    + case when objectproperty(trig.object_id, 'ExecIsDeleteTrigger') = 1 
            then 'Delete ' else '' end
    + case when objectproperty(trig.object_id, 'ExecIsInsertTrigger') = 1 
            then 'Insert ' else '' end
    ) as [event],
    case when trig.[type] = 'TA' then 'Assembly (CLR) trigger'
        when trig.[type] = 'TR' then 'SQL trigger' 
        else '' end as [type],
    case when is_disabled = 1 then 'Disabled'
        else 'Active' end as [status],
		left(object_definition(trig.object_id),70)+CASE when LEN(object_definition(trig.object_id))>70 THEN '...' ELSE '' END AS definition, 
        checksum(object_definition(trig.object_id)) AS hash
from sys.triggers trig
    inner join sys.objects tab
        on trig.parent_id = tab.object_id
order by schema_name(tab.schema_id) + '.' + tab.name, trig.name
FOR JSON auto
'@ | ConvertFrom-Json
					
            <# RDBMS  #>
					#$ErrorActionPreference='Stop' 
					$THeTypes = $TheRelationMetadata | Select schema, type -Unique
					if ($Routines -ne $null)
					{ $TheTypes = $THeTypes + $Routines | Select schema, type -Unique }
            <# OK. we now have to assemble all this into a model that is as human-friendly as possible  #>
					$SchemaTree = @{ } <# This will become our model of the schema. Fist we put in
            all the types of relations  #>
					
					
					$TheTypes | Select -ExpandProperty schema -Unique | foreach{
						$TheSchema = $_;
						$ourtypes = @{ }
						$TheTypes | where { $_.schema -eq $TheSchema } | Select -ExpandProperty type | foreach{ $OurTypes += @{ $_ = @{ } } }
						$SchemaTree | add-member -NotePropertyName $TheSchema -NotePropertyValue $OurTypes
					}
					
            <# now inject all the objects into the schema tree. First we get all the relations  #>
					$TheRelationMetadata | Select schema, type, name -Unique | foreach{
						$schema = $_.schema;
						$type = $_.type;
						$object = $_.name;
						$TheColumnList = $TheRelationMetadata |
						where { $_.schema -eq $schema -and $_.type -eq $type -and $_.name -eq $object } -OutVariable pk |
						foreach{ $_.column }
						$SchemaTree.$schema.$type += @{ $object = @{ 'columns' = $TheColumnList } }
					}
					#display-object $schemaTree|convertto-json -depth 10
            <# now stitch in the constraints with their columns  #>
					$constraints | Select schema, table_name, Type, constraint_name, referenced_table -Unique | foreach{
						$constraintSchema = $_.schema;
						$constrainedTable = $_.table_name;
						$constraintName = $_.constraint_name;
						$ConstraintType = $_.type;
						$referenced_table = $_.referenced_table;
						$definition = $_.definition;
						# get the original object
						if ($ConstraintType -notin @('Unique key', 'Primary key', 'foreign key'))
						{ $SchemaTree.$constraintSchema.table.$constrainedTable.$ConstraintType = @{ $constraintName = $definition } }
						else
						{
							#we have to deal with columns
							$OriginalConstraint = $constraints |
							where{
								$_.schema -eq $constraintSchema -and
								$_.table_name -eq $constrainedTable -and
								$_.Type -eq $ConstraintType -and
								$_.constraint_name -eq $constraintName
							} | Select -first 1
							$Columns = $OriginalConstraint | Sort-Object -Property ordinal_position |
							Select -ExpandProperty column_name
							if ($ConstraintType -eq 'foreign key')
							{
								$Referencing = $OriginalConstraint | Sort-Object -Property ordinal_position |
								Select -ExpandProperty referenced_column
								$SchemaTree.$constraintSchema.table.$constrainedTable.$ConstraintType += @{
									$constraintName = @{ 'Cols' = $columns; 'Foreign Table' = $referenced_table; 'Referencing' = "$Referencing" }
								}
							}
							elseif ($ConstraintType -in @('Unique key', 'Primary key'))
							{ $SchemaTree.$constraintSchema.table.$constrainedTable.$ConstraintType += @{ $constraintName = $columns } }
							
						}
						
					}
					
					
            <# now stitch in the indexes with their columns  #>
					$indexes | Select schema, table_name, Type, index_name -Unique | foreach{
						$indexSchema = $_.schema;
						$indexedTable = $_.table_name;
						$indexName = $_.index_name;
						$columns = $indexes |
						where{
							$_.schema -eq $indexSchema -and
							$_.table_name -eq $indexedTable -and
							$_.index_name -eq $indexName
						} | Sort-Object -Property key_ordinal | Select -ExpandProperty name
						$SchemaTree.$indexSchema.table.$indexedTable.index += @{ $indexName = @{ 'Indexing' = $columns } }
					}
					$PSDefaultParameterValues['Out-File:Encoding'] = 'utf8'
					$SchemaTree | convertTo-json -depth 10 > "$MyOutputReport"
					$SchemaTree | convertTo-json -depth 10 > "$MycurrentReport"
					
					
				} #end SQL Server
				default
				{
					$Param1.Problems.'SavedDatabaseModelIfNecessary' += "The $_ database isn't supported yet. Sorry about that."
				}
			}
			#Final things to do: Break up the model into individual objects, in different folders depending on type
			if (Test-Path "$MyOutputReport" -PathType leaf)
			{
				$PSDefaultParameterValues['Out-File:Encoding'] = 'utf8' #we'll need it
				$Model = [IO.File]::ReadAllText("$MyOutputReport") | ConvertFrom-JSON
				$model.psobject.Properties |
				foreach{ $schema = $_.Name; $_.Value.psobject.Properties } |
				Foreach{ $Type = $_.Name.ToLower(); $_.Value.psobject.Properties } |
				foreach{
					$objectName = $_.Name;
					$WhereToStoreIt = "$MyModelPath\$type"
					if (-not (Test-Path "$WhereToStoreIt" -PathType Container))
					{ $null = New-Item -ItemType directory -Path "$WhereToStoreIt" -Force }
					$_.Value | convertto-json > "$WhereToStoreIt\$schema.$objectName.json"
					Copy-Item -Path "$MyModelPath" -Destination "$MyCurrentPath" -Recurse -Force
				}
				$feedback += "written object-level model to $MyModelPath"
			}
		}
		catch { $problems += "$($PSItem.Exception.Message)" }
	}
	else
	{
		$AlreadyDone = $true;
		$feedback += "Nothing to do. The model is already there in '$MyOutputReport'"
	}
	if ($problems.Count -gt 0)
	{
		$Param1.Problems.'SaveDatabaseModelIfNecessary' += $problems;
	}
	else
	{
		if ($AlreadyDone)
		{
			$Param1.Feedback.'SaveDatabaseModelIfNecessary' = "$MyOutputReport already exists"
		}
		else
		{
			if ($feedback.count -gt 0)
			{ $Param1.Feedback.'SaveDatabaseModelIfNecessary' = $feedback }
			$Param1.WriteLocations.'SaveDatabaseModelIfNecessary' = $MyOutputReport;
		}
	}
	
}


<# this creates a first-cut UNDO script for the metadata (not the data) which can
be adjusted and modified quickly to produce an UNDO Script. It does this by using
SQL Compare to generate a  idepotentic script comparing the database with the 
contents of the previous version.#>
$CreateUndoScriptIfNecessary = {
	Param ($param1) # $CreateUndoScriptIfNecessary (Don't delete this) 
	$problems = @(); # well, not yet
    $feedback= @(); # well, nothing yet
    $WeCanDoIt=$true; #assume that we can generate a script ....so far!
    #check that we have values for the necessary details
	@('version', 'server', 'database', 'project') |
	foreach{ if ($param1.$_ -in @($null,'')) { $Problems += "no value for '$($_)'" } }
    $command=$null;
    $command = get-command SQLCompare -ErrorAction Ignore
    f ($command -eq $null) 
        {
    	if ($SQLCompareAlias-ne $null)
            {Set-Alias SQLCompare $SQLCompareAlias}
        else
            {$problems += 'You must have provided a path to SQL Compare in the ToolLocations.ps1 file in the resources folder'}
    }	#the database scripts path would be up to you to define, of course
    $scriptsPath= if ([string]::IsNullOrEmpty($param1.scriptsPath)) {'scripts'} else {"$($param1.scriptsPath)"}
    $sourcePath= if ([string]::IsNullOrEmpty($param1.sourcePath)) {'Source'} else {"$($param1.SourcePath)"}
    $EscapedProject=($Param1.project.Split([IO.Path]::GetInvalidFileNameChars()) -join '_') -ireplace '\.','-'
   #What was the previous version?
    if ($param1.Previous -in ($null,'0.0.0')) {
        $feedback += "no previous version to undo to"; 
        $null=New-Item -ItemType Directory -Force "$env:Temp\DummySource"
        $PreviousDatabasePath="$env:Temp\DummySource";
        }
    else
        {
        $PreviousDatabasePath = 
        if  ($param1.directoryStructure -in ('classic',$null)) #If the $ReportDirectory has a value
          {"$($env:USERPROFILE)\$($param1.Reportdirectory)$($escapedProject)\$($param1.Previous)\$sourcePath"} 
        else {"$($param1.reportLocation)\$($param1.Previous)\$sourcePath"} #else the simple version

        If (!(Test-Path -path $PreviousDatabasePath -PathType Container)) 
            {$WeCanDoIt=$False} #Because no previous source
        } 
    $CurrentUndoPath = 
        if  ($param1.directoryStructure -in ('classic',$null)) #If the $ReportDirectory has a value
          {"$($env:USERPROFILE)\$($param1.Reportdirectory)$($escapedProject)\$($param1.Version)\$scriptsPath"} 
        else {"$($param1.reportLocation)\$($param1.Version)\$scriptsPath"} #else the simple version
    if (Test-Path -Path "$CurrentUndoPath\U$($Param1.Version)__Undo.sql" -PathType Leaf )
        {$WeCanDoIt=$False} #Because it has already been done             
    If ($WeCanDoIt)
	    {
        $CLIArgs = @(# we create an array in order to splat the parameters. With many command-line apps you
		    # can use a hash-table 
            "/Scripts1:$PreviousDatabasePath"
		    "/server2:$($param1.server)",
            '/include:identical',#a migration may just be data, no metadata.
		    "/database2:$($param1.database)",
		    "/force", # 
		    "/options:NoErrorHandling,IgnoreQuotedIdentifiersAndAnsiNullSettings,NoTransactions,DoNotOutputCommentHeader,ThrowOnFileParseFailed,ForceColumnOrder,IgnoreNoCheckAndWithNoCheck,IgnoreSquareBrackets,IgnoreWhiteSpace,ObjectExistenceChecks,IgnoreSystemNamedConstraintNames,IgnoreTSQLT,NoDeploymentLogging", 
# so that we can use the script with Flyway more easily
		    "/LogLevel:Warning",
		    "/ScriptFile:$CurrentUndoPath\U$($Param1.Version)__Undo.sql"
	    )
	
	    if ($param1.uid -ne $NULL) #add the arguments for credentials where necessary
	    {
		    $CLIArgs += @(
			    "/username2:$($param1.uid)",
			    "/Password2:$($param1.pwd)"
		    )
	    }
         if ($param1.'filterpath' -ne $NULL) #add the arguments for compare filters
		{
			$CLIArgs += @(
				"/filter:$($param1.filterpath)"
			)
         }
        else
            {
            $CLIArgs += @(
              "/exclude:table:$($param1.flywayTable)")
		} 
	    if (-not (Test-Path -PathType Container $CurrentUndoPath))
	    {
		    # is the path to the scripts directory
		    # not there, so we create the directory 
		    $null = New-Item -ItemType Directory -Force $CurrentUndoPath;
	    }
		# if it is done already, then why bother? (delete it if you need a re-run for some reason 	
		Sqlcompare @CLIArgs #run SQL Compare with splatted arguments
		if ($LASTEXITCODE-eq 63) { "no changes to the metadata between ($param1.Previous) and this version $($param1.Version) of $($param1.Project)" }
		if ($?) { "Written build script for $($param1.Project) $($param1.Version) to $MyDatabasePath" }
		else # if no errors then simple message, otherwise....
		{
			#report a problem and send back the args for diagnosis (hint, only for script development)
			$Arguments = '';
			$Arguments += $CLIArgs | foreach{ $_ }
			$Problems += "SQLCompare Went badly. (code $LASTEXITCODE) with paramaters $Arguments"
		}
		if ($problems.count -gt 0)
		{ $Param1.Problems.'CreateUNDOScriptIfNecessary' += $problems; }
	    else
	    {
	    $Param1.WriteLocations.'CreateUNDOScriptIfNecessary' = "$CurrentUndoPath\U$($Param1.Version)__Undo.sql";
	    }

	}
	else {
          $feedback+= "This version '$($param1.Version)' already has a undo script to get to $($param1.Previous) at $CurrentUndoPath\U$($Param1.Version)__Undo.sql "
         }
	if ($feedback.count -gt 0)
		{$Param1.feedback.'CreateUNDOScriptIfNecessary' = $feedback}
	
}

<# this creates a first-cut migration script for the metadata (not the data) which can
be adjusted, documented  and modified quickly to produce an migration Script. It does this by using
SQL Compare to generate a  idepotentic script comparing contents of the current version of the database with the 
 live version. Do not do this within a Flyway Session (such as an 'afterEach')#>
$CreatePossibleMigrationScript = {
	Param ($param1) # $CreatePossibleMigrationScript (Don't delete this) 
	$problems = @(); # well, not yet
     #check that we have values for the necessary details
	@('version', 'server', 'database', 'project') |
	foreach{ if ($param1.$_ -in @($null,'')) { $Problems += "no value for '$($_)'" } }
	# the alias must be set to the path of your installed version of SQL Compare
    $command=$null;
    $command = get-command SQLCompare -ErrorAction Ignore 
    if ($command -eq $null) {
    	if ($SQLCompareAlias-ne $null)
            {Set-Alias SQLCompare $SQLCompareAlias -Scope Script}
        else
            {$problems += 'You must have provided a path to SQL Compare in the ToolLocations.ps1 file in the resources folder'}
    }	#the database scripts path would be up to you to define, of course
    $scriptsPath= if ([string]::IsNullOrEmpty($param1.scriptsPath)) {'scripts'} else {"$($param1.scriptsPath)"}
    $sourcePath= if ([string]::IsNullOrEmpty($param1.sourcePath)) {'Source'} else {"$($param1.SourcePath)"}
    $EscapedProject=($Param1.project.Split([IO.Path]::GetInvalidFileNameChars()) -join '_') -ireplace '\.','-'
    $CurrentVersionPath = 
        if  ($param1.directoryStructure -in ('classic',$null)) #If the $ReportDirectory has a value
          {"$($env:USERPROFILE)\$($param1.Reportdirectory)$($escapedProject)\$($param1.Version)"} 
        else {"$($param1.reportLocation)\$($param1.Version)"} #else the simple version
 	
    $CLIArgs = @(# we create an array in order to splat the parameters. With many command-line apps you
		# can use a hash-table 
        "/Scripts2:$CurrentVersionPath\$SourcePath"
		"/server1:$($param1.server)",
        '/include:identical',#a migration may just be data, no metadata.
		"/database1:$($param1.database)",
        "/report:$CurrentVersionPath\Drift.xml",
        "/reportType:XML"
		"/force", # 
		"/options:NoErrorHandling,IgnoreExtendedProperties,IgnoreQuotedIdentifiersAndAnsiNullSettings,NoTransactions,DoNotOutputCommentHeader,ThrowOnFileParseFailed,ForceColumnOrder,IgnoreNoCheckAndWithNoCheck,IgnoreSquareBrackets,IgnoreWhiteSpace,ObjectExistenceChecks,IgnoreSystemNamedConstraintNames,IgnoreTSQLT,NoDeploymentLogging", 
# so that we can use the script with Flyway more easily
		"/LogLevel:Warning",
		"/ScriptFile:$CurrentVersionPath\$scriptsPath\MigrationFrom$($param1.Version)ToNextVersion.sql"
	)
	
	if ($param1.uid -ne $NULL) #add the arguments for credentials where necessary
	{
		$CLIArgs += @(
			"/username1:$($param1.uid)",
			"/Password1:$($param1.pwd)"
		)
	}
    if ($param1.'filterpath' -ne $NULL) #add the arguments for compare filters
	{
		$CLIArgs += @(
			"/filter:$($param1.filterpath)"
		)
        }
    else
        {
        $CLIArgs += @(
            "/exclude:table:$($param1.flywayTable)")
	} 
	if (-not (Test-Path -PathType Container $CurrentVersionPath))
	{
		# is the path to the scripts directory
		# not there, so we create the directory 
		$null = New-Item -ItemType Directory -Force $CurrentVersionPath;
	}
	# if it is done already, then why bother? (delete it if you need a re-run for some reason 	
	Sqlcompare @CLIArgs #run SQL Compare with splatted arguments
	if ($LASTEXITCODE-eq 63) {$param1.feedback.'CreatePossibleMigrationScript'= "There have been no changes to version $($param1.Version) of $($param1.Project)" }
	if ($?) { "Written build script for $($param1.Project) $($param1.Version) to $MyDatabasePath" }
	else # if no errors then simple message, otherwise....
	{
		#report a problem and send back the args for diagnosis (hint, only for script development)
		$Arguments = '';
		$Arguments += $CLIArgs | foreach{ $_ }
		$Problems += "SQLCompare Went badly. (code $LASTEXITCODE) with paramaters $Arguments"
	}
	if ($problems.count -gt 0)
	{ $Param1.Problems.'CreatePossibleMigrationScript' += $problems; }
	else
	{
	$Param1.WriteLocations.'CreatePossibleMigrationScript' = "$CurrentVersionPath\$scriptsPath\MigrationFrom$($param1.Version)ToNextVersion.sql";
    $Param1.feedback.'CreatePossibleMigrationScript'="A migration file has been created from the present version ($($param1.Version))" 
	}
	
}


<#
This script performs a bulk copy operation to get data into a database. It
can only do this if the data is in a suitable directory. At the moment it assumes
that you are using a DATA directory at the same level as the scripts directory. 
BCP must have been previously installed in the path 
Unlike many other tasks, you are unlikely to want to do this more than once for any
database.If you did, you'd need to clear out the existing data first! It is intended
for static scripts AKA baseline migrations.
#>
$BulkCopyIn = {
	Param ($param1) # $BulkCopyIn (Don't delete this) 
	$problems = @(); # well, not yet
	$WeCanDoIt = $true; #assume that we can BCP data in.so far!
	#check that we have values for the necessary details
	@('server', 'database', 'project', 'version') |
	foreach{ if ($param1.$_ -in @($null, '')) { $Problems += "no value for '$($_)'" } }
    if ([string]::IsNullOrEmpty($param1.dataPath)) #has he specified a datapath
            {$FilePath = '..\Data'}
	    else
            {$FilePath = "..\$($param1.datapath)"};
	
	#Now finished getting credentials. Is the data directory there
	if (!(Test-Path -path $Filepath -PathType Container))
	{
		$Problems += 'No appropriate directory with bulk files yet';
		$weCanDoIt = $false;
	}
	if ($weCanDoIt)
	{
		#now we know the version we get a list of the tables.
		$Tables = $GetdataFromSQLCMD.Invoke($Param1, @"
SELECT Object_Schema_Name (object_id) AS [Schema], name
     FROM sys.tables
     WHERE
     is_ms_shipped = 0 AND name NOT LIKE 'Flyway%'
  FOR JSON AUTO
"@) | ConvertFrom-Json
		Write-verbose "Reading data in from $DirectoryToLoadFrom"
		if ($Tables.Error -ne $null)
		{
			$internalLog += $Tables.Error;
			$weCanDoIt = $false;
		}
	}
	
	$Result = $GetdataFromSQLCMD.Invoke($Param1, @'
    EXEC sp_MSforeachtable "ALTER TABLE ? NOCHECK CONSTRAINT all"
'@);
	if ($Result.Error -ne $null)
	{
		$internalLog += $Tables.Error;
		$weCanDoIt = $false;
	}
	
	if ($weCanDoIt)
	{
		$directory = "$Filepath\$($version.Split([IO.Path]::GetInvalidFileNameChars()) -join '_')";
		$Tables |
		foreach {
			# calculate where it gotten from #
			$filename = "$($_.Schema)_$($_.Name)".Split([IO.Path]::GetInvalidFileNameChars()) -join '_';
			$progress = '';
			Write-Verbose "Reading in $filename from $($directory)\$filename.bcp"
			if ($User -ne '') #using standard credentials 
			{
				$Progress = BCP "$($_.Schema).$($_.Name)" in "$directory\$filename.bcp" -q -n -E `
								"-U$($user)"  "-P$password" "-d$($Database)" "-S$server"
			}
			else #using windows authentication
			{
				#-E Specifies that identity value or values in the imported data are to be used
				$Progress = BCP "$($_.Schema).$($_.Name)" in "$directory\$filename.bcp" -q -n -E `
								"-d$($Database)" "-S$server"
			}
			if (-not ($?) -or $Progress -like '*Error*') # if there was an error
			{
				$Problems += "Error with data import  of $($directory)\$($_.Schema)_$($_.Name).bcp -  $Progress ";
			}
		}
	}
	$Result = $GetdataFromSQLCMD.Invoke($Param1, @'
ALTER TABLE ? WITH CHECK CHECK CONSTRAINT all
'@)
	if ($problems.count -gt 0)
	{ $Param1.Problems.'BulkCopyIn' += $problems; }
	
}

<#
This script performs a bulk copy operation to get data out of a database, and
into a suitable directory. At the moment it assumes that you wish to use a 
DATA directory at the same level as the scripts directory. 
BCP must have been previously installed in the path.
#>
$BulkCopyOut = {
	Param ($param1) # $BulkCopyOut (Don't delete this) 
	$problems = @(); # well, not yet
	$WeCanDoIt = $true; #assume that we can BCP data in.so far!
	#check that we have values for the necessary details
	@('version', 'server', 'database', 'project') |
	foreach{ if ($param1.$_ -in @($null, '')) { $Problems += "no value for '$($_)'" } }
	if ([string]::IsNullOrEmpty($param1.dataPath)) #has he specified a datapath
        {$FilePath = '..\Data'}
	else
        {$FilePath = "..\$($param1.datapath)"}
	if (!(Test-Path -path $Filepath -PathType Container))
	{
		$Null = New-Item -ItemType Directory -Path $FilePath -Force
	}
	
	#now we know the version we get a list of the tables.
	$Tables = $GetdataFromSQLCMD.Invoke($param1, @"
SELECT Object_Schema_Name (object_id) AS [Schema], name
     FROM sys.tables
     WHERE
     is_ms_shipped = 0 AND name NOT LIKE 'Flyway%'
  FOR JSON AUTO
"@) | ConvertFrom-Json
	Write-verbose "Reading data in from $DirectoryToLoadFrom"
	if ($Tables.Error -ne $null)
	{
		$internalLog += $Tables.Error;
		$WeCanDoIt = $false;
	}
	if ($WeCanDoIt)
	{
		$directory = "$Filepath\$($version.Split([IO.Path]::GetInvalidFileNameChars()) -join '_')";
        if (-not (Test-Path "$directory" -PathType Container))
        { New-Item -ItemType directory -Path "$directory" -Force}        
		$Tables |
		foreach {
			# calculate where it gotten from #
			$filename = "$($_.Schema)_$($_.Name)".Split([IO.Path]::GetInvalidFileNameChars()) -join '_';
			$progress = '';
			Write-Verbose "writing out $filename to  $($directory)\$filename.bcp"
			if ($User -ne '') #using standard credentials 
			{
				$Progress = BCP "$($_.Schema).$($_.Name)"  out  "$directory\$filename.bcp"  `
								-n "-d$($Database)"  "-S$($server)"  `
								"-U$user" "-P$password"
			}
			else #using windows authentication
			{
				#-E Specifies that identity value or values in the imported data are to be used
				
				$Progress = BCP "$($_.Schema).$($_.Name)" in "$directory\$filename.bcp" -q -n -E `
								"-d$($Database)" "-S$server"
				
			}
			if (-not ($?) -or $Progress -like '*Error*') # if there was an error
			{
				$Problems += "Error with data export  of $($directory)\$($_.Schema)_$($_.Name).bcp -  $Progress ";
			}
		}
	}
	if ($problems.count -gt 0)
	{ $Param1.Problems.'BulkCopyOut' += $problems; }


}


<#
This script creates a PUML file for a Gantt chart at the current version of the database. This can be
read into any editor that takes PlantUML files to give a Gantt chart
#>
$GeneratePUMLforGanttChart = {
	Param ($param1) # $GeneratePUMLforGanttChart (Don't delete this) 
	$problems = @(); #no problems so far
	#check that you have the  entries that we need in the parameter table.
	@('server', 'database', 'project', 'uid') | foreach{
		if ([string]::IsNullOrEmpty($Param1.$_))
		{ $Problems += "no value for '$($_)'" }
	}
    $EscapedProject=($Param1.project.Split([IO.Path]::GetInvalidFileNameChars()) -join '_') -ireplace '\.','-'
	$MyDatabasePath =
	if  ($param1.directoryStructure -in ('classic',$null)) #If the $ReportDirectory has a value
	{ "$($env:USERPROFILE)\$($param1.Reportdirectory)$($escapedProject)\$($param1.Version)\Reports" }
	else { "$($param1.reportLocation)\$($param1.Version)\Reports" } #else the simple version
	$flywayTable = $Param1.flywayTable
	if ($flywayTable -eq $null)
	{ $flywayTable = 'dbo.flyway_schema_history' }
	if (-not (Test-Path -PathType Container $MyDatabasePath))
	{
		# does the path to the reports directory exist?
		# not there, so we create the directory 
		$null = New-Item -ItemType Container -Force $MyDatabasePath;
	}
    $Puml=''
	if ($problems.Count -eq 0)
	{
		$puml = $GetdataFromSQLCMD.Invoke($param1, @"
/* we read the Flyway Schema History into a table variable so we can then do a line--by-line select with 
a guarantee of doing it in the order of the primary key */
set nocount on
DECLARE @FlywaySchemaTable TABLE
   ([installed_rank] [INT] NOT NULL PRIMARY KEY,
   [version] [NVARCHAR](50) NULL,
   [description] [NVARCHAR](200) NULL,
   [installed_by] [NVARCHAR](100) NOT NULL,
   [installed_on] [DATETIME] NOT NULL)
/* now read in the table */
INSERT INTO @FlywaySchemaTable
  (Installed_rank, version,  
   installed_by, installed_on, description )
   --I've added the placeholders in case you want to execute this in a callback
SELECT fsh.installed_rank, version, installed_by, installed_on, description
  FROM $flywayTable FSH 
    INNER JOIN
      (SELECT  Max (installed_rank) AS installed_rank
         FROM $flywayTable 
         WHERE
         success = 1 AND type = 'SQL' AND version IS NOT NULL
         GROUP BY version) f
      ON f.installed_rank = fSH.installed_rank
  ORDER BY fSH.installed_rank;

/* now we calculate the version. This is slightly complicated by the
possibility that you've done an UNDO. I've added the placeholders
in case you want to execute this in a callback */
DECLARE @Version [NVARCHAR](50) =
    (SELECT TOP 1 [version] --we need to find the greatest successful version.
        FROM $flywayTable -- 
        WHERE
        installed_rank =
        (SELECT Max (installed_rank)
            FROM $flywayTable 
            WHERE success = 1));

DECLARE @PlantUMLCode NVARCHAR(MAX)='@startgantt
skinparam LegendBorderRoundCorner 2
skinparam LegendBorderThickness 1
skinparam LegendBorderColor silver
skinparam LegendBackgroundColor white
skinparam LegendFontSize 11
printscale weekly
saturday are closed
sunday are closed
title Gantt Chart for version '+@Version+'
legend top left
  Database: '+Db_Name()+'
  Server: '+@@ServerName+'
  RDBMS: sqlserver
  Flyway Version: '+@Version+'
endlegend
printscale weekly
saturday are closed
sunday are closed
'
DECLARE @PreviousDescription NVARCHAR(100) 
--used to temporarily hold the previous description

SELECT @PlantUMLCode=@PlantUMLCode + 
  CASE WHEN @PreviousDescription IS NULL THEN 'Project starts '+Convert(NCHAR(11),Convert(DATETIME2,Installed_on,112)) +'
' ELSE '' END+
'['+version+' - '+description+'] on {'+[installed_by]+'} starts '+ Convert(NCHAR(11),Convert(DATETIME2,Installed_on,112))+'
' 
+ CASE WHEN @PreviousDescription IS NOT NULL THEN '['+@Previousdescription+'] ends '+Convert(NCHAR(11),Convert(DATETIME2,Installed_on,112))+'
' ELSE '' END,
      @PreviousDescription = version+' - '+description
FROM @FlywaySchemaTable WHERE version IS NOT null
SELECT @PlantUMLCode=@PlantUMLCode+'@endgantt'
SELECT @PlantUMLCode

"@, $null, $true)
		
		[IO.File]::WriteAllLines("$MyDatabasePath\GanttChart.puml", $puml) # It must be UTF8!!!
	}
if ($problems.count -gt 0)
	{ $Param1.Problems.'GeneratePUMLforGanttChart' += $problems; 
	}
	else
	{
	$Param1.WriteLocations.'GeneratePUMLforGanttChart' = "$MyDatabasePath\GanttChart.puml";
	}

}
<# create a markdown report of what happened in the last migration  
$param1=$param1
#>

$CreateVersionNarrativeIfNecessary = {
	Param ($param1) # $CreateVersionNarrativeIfNecessary - dont delete this
	$problems = @() #none yet!
    $warnings =@()
    $feedback =@()
    $unnecessary=$false;
    $AlreadyDone=$false;
    $GoodVersion=$true;
    #check that you have the  entries that we need in the parameter table.
    try
        {
	    @( 'version', 'previous', 'project') | foreach{
		    if ([string]::IsNullOrEmpty($param1.$_))
		    { $Problems += "no value for '$($_)'" }
	    }
	#calculate the report paths
	    $escapedProject = ($Param1.Project.Split([IO.Path]::GetInvalidFileNameChars()) -join '_') -ireplace '\.', '-'
	
	    if ($param1.directoryStructure -in ('classic', $null)) #If the $ReportDirectory has a value
	    {
		    $PreviousVersionReportPath =
		    "$($env:USERPROFILE)\$($param1.Reportdirectory)$($escapedProject)\$($param1.Previous)\Reports"
		    $currentVersionReportPath =
		    "$($env:USERPROFILE)\$($param1.Reportdirectory)$($escapedProject)\$($param1.version)\Reports"
	    }
	    else
	    {
		    $PreviousVersionReportPath = "$($param1.reportLocation)\$($param1.previous)\Reports"
		    $currentVersionReportPath = "$($param1.reportLocation)\$($param1.Version)\Reports"
		
	    }
        if (-not (Test-Path "$PreviousVersionReportPath" -PathType Container))
            { New-Item -ItemType directory -Path "$PreviousVersionReportPath" -Force}
        if (-not (Test-Path "$currentVersionReportPath" -PathType Container))
            { New-Item -ItemType directory -Path "$currentVersionReportPath" -Force}
	}
	catch
	{
		$problems += "$($PSItem.Exception.Message)"
	}

	if ($param1.Version -eq '0.0.0') { $warnings += "Cannot compare an empty database with anything" }
	$GoodVersion = try { $null = [Version]$param1.Version; $true }
	catch { $false }
	if (-not ($goodVersion))
	{ $problems += "Bad version number '$($param1.Version)'" }
	
	if ($goodVersion)
	{
		if ($param1.previous -eq '0.0.0') {
          $feedback += "Cannot compare with an empty database";
          $unnecessary=$true
           }
		$GoodVersion = try { $null = [Version]$param1.previous; $true }
		catch { $false }
		if (-not ($goodVersion))
		{ $problems += "Bad previous version number '$($param1.Version)'" }
	}
	
	if ($goodVersion)
	{
		If (Test-Path -Path "$currentVersionReportPath\VersionNarrative.MD")
		{
			$AlreadyDone = $true
		}
		If (!(Test-Path -Path "$PreviousVersionReportPath\DatabaseModel.JSON"))
		{
			$feedback += "No database model exists for '$($param1.Previous)'"
            $unnecessary=$true;
		}
		If (!(Test-Path -Path "$currentVersionReportPath\DatabaseModel.JSON"))
		{
			$warnings += "No database model exists for '$($param1.version)'"
		}
		if ($problems.Count -eq 0 -and $warnings.count  -eq 0 -and !$unnecessary -and !$AlreadyDone)
		{
			try
			{
				#see what is changed by comparing the models before and after the migration
				$Comparison = Diff-Objects -Parent $Details.Database  -depth 10 <#get the previous model from file#>`
				([IO.File]::ReadAllText("$PreviousVersionReportPath\DatabaseModel.JSON") |
					ConvertFrom-JSON)<#and get the current model from file#> `
				([IO.File]::ReadAllText("$currentVersionReportPath\DatabaseModel.JSON") |
					ConvertFrom-JSON) |
				where { $_.match -ne '==' }
				$Comparison | Export-CSV -Path "$currentVersionReportPath\MetadataChanges.report"
                $PSDefaultParameterValues['Out-File:Encoding'] = 'utf8';
				($Comparison | ConvertTo-JSON) > "$currentVersionReportPath\MetadataChanges.json"
				#we can do all sorts of more intutive reports from the output
                $ObjectName='nomatch';
                $narrative=@()
                $Narrative+="### Alterations to project $($param1.projectName), database $($param1.database), in version $($param1.version)`n"
				$narrative+=$Comparison | foreach{
					$current = $_;
                    $objectList=$current.Ref -split '.'
                    switch ($current.Match)
					{
						'->' {
							  if ($current.Target -ne '(PSCustomObject)')
                               {
                                   if ($current.ref -like "$($ObjectName)*")
								    { "  - Added $($current.Target) to $($current.Ref -replace $ObjectName,'' )`n " }
                                    else
                                    {"- Added '$($current.Target -replace $ObjectName,'' )' to '$($current.Ref)'`n ";
                                      $ObjectName='nomatch';
                                    }
                                }
							else {
                                     "- Added  '$($current.Ref)'`n";
                                     $objectName=$current.Ref;
						         }
                            }
						'<>' {
							"- Changed '$($current.Source)' to '$($current.Target
							)' at '$($current.Ref)`n"
						}
						'<-' {
                             if ($current.Source -eq '(PSCustomObject)') 
							{"- Deleted  '$($current.Ref
							) `n"}
                            
						}
						default { "No metadata change `n" }
					}
                }
            $PSDefaultParameterValues['Out-File:Encoding'] = 'utf8';    					
			$Narrative > "$currentVersionReportPath\VersionNarrative.MD"

			}
			catch
			{
				$problems += "$($PSItem.Exception.Message)"
			}
		}
		if ($problems.Count -gt 0)
		{
			$Param1.Problems.'CreateVersionNarrativeIfNecessary' += $problems;
		}
		elseif ($warnings.Count -gt 0)
		{
			$Param1.warnings.'CreateVersionNarrativeIfNecessary' += $warnings;
		}
		else
		{
			if ($AlreadyDone)
			{
				$Param1.Feedback.'CreateVersionNarrativeIfNecessary' = "$Version narrative for $($param1.version) already exists"
			}
			if ($unnecessary)
			{
				$Param1.Feedback.'CreateVersionNarrativeIfNecessary' += "$Version narrative for $($param1.version) isn't necessary"
			}
			else
			{
				$Param1.WriteLocations.'CreateVersionNarrativeIfNecessary' = "$currentVersionReportPath\VersionNarrative.MD";
			}
		}
	}
}

<#
This creates a simple entity diagram for the current version. You only need two files to do this and
you don't need to contact the database. The ER diagram has all objects that are either added, removed or
changed colour-coded so you can see immediately what has changed. The idea of this is to be able to paste
the resulting SVG file or other image file of the diagram, produced by PlantUMLc.exe.
#>

$WriteOutERDiagramCode = {
	Param ($param1,
		$version = $Null,
		#the flyway version of the database. Leave null if using framework
		$Title = $null,
        #The shared directory where all the files are read or written to
        $FileLocations = $null,
		#the flyway project. Leave null if using framework
		$MetadatachangeFile = $null,
		#Specify if not using the default location
		$modelFile = $null,
		#Specify if not using the default location
		$MyPUMLFile = $null) # $WriteOutERDiagramCode - dont delete this
	
	
	$PSDefaultParameterValues['Out-File:Encoding'] = 'utf8' #we'll be using out redirection
	$problems = @() #none yet!
	$feedback = @();
	if ($version -eq $null)
	{ $version = $param1.version };
	#the flyway version of the database. Leave null if using framework
	if ($Title -eq $null)
	{ $Title = "$($Param1.project) database: $($Param1.Database) $version $($Param1.branch) branch" };
	#the flyway project. Leave null if using framework
    if ($FileLocations -eq $null) # just in case you wish to specify a shard location
	{ $FileLocations = "$($Param1.reportLocation)\$version\Reports" };
	if ($MetadatachangeFile -eq $null)
	{ $MetadatachangeFile = "$FileLocations\MetadataChanges.json" }
	if ($modelFile -eq $null)
	{ $modelFile = "$FileLocations\DatabaseModel.JSON" }
	if ($MyPUMLFile -eq $null)
	{ $MyPUMLFile = "$FileLocations\ERDiagram.PUML" }
	
	# variables to colour changed, added (or deleted database objects)
	$deleted = '#pink ##[bold]red'
	$added = '#lightgreen ##[bold]green'
	$altered = '#gold ##[bold]saddlebrown'
	
	# check that the files exist
	@($MetadatachangeFile, $modelFile) | foreach{
		If (!(Test-Path -PathType Leaf -Path $_))
		{
			$problems += "$_ does not exist. You'll need this for this diagram"
		}
	}
	
	if ($problems.Count -eq 0)
	{
<# First, get the change file and convert it to a PoSh object#>
		$changes = ConvertFrom-json ([IO.File]::ReadAllText($MetadatachangeFile))
		if ($changes -ne $null)
		{
			$ChangedEntities = $changes | foreach{ @{ 'object' = $_.Ref.Split('.'); 'match' = $_.match } }
			$ChangedObjects = @{ }
			$ChangedEntities | foreach {
				"$($_.object[1]).$($_.object[3])" } | sort -Unique | foreach{ $ChangedObjects."$($_)" = '--' }
			$ChangedEntities | foreach {
				@{ "$($_.object[1]).$($_.object[3])" = $_.match } } |
			foreach{
				$which = $_;
				$change = [string]$which.Values[0]
				$objectName = [string]$which.Keys[0]
				$ChangedObjects."$objectName" =
				switch ($ChangedObjects."$objectName")
				{
					'--' { "$change" }
					'<-' { '<' + $change[1] }
					'->' { $change[0] + '>' }
					default { '<>' }
				}
			}
		}
		
<# Get the model and convert it into a PlantUML script #>
		$Model = ConvertFrom-json ([IO.File]::ReadAllText($modelFile))
		$EntityCode = $model | Display-object -depth 3 |
		foreach{
			#split into the individual objects
			$bits = $_.Path.split('.');
			$objectName = "$($Bits[1]).$($bits[3])"
			$ObjecType = "$($Bits[2].replace(' ', '_'))"
			[pscustomobject]@{
				'ObjectName' = $objectName; 'ObjectType' = $ObjecType;
				'change' = if ($changes -ne $null)
				{
					$TheObjectName = "$($Bits[1]).$($bits[3])"
					if ($Changedobjects.$TheObjectName -ne $null)
					{
						switch ($Changedobjects.$TheObjectName)
						{
							'->' { $added } '<-' { $deleted }  '<>' { $altered }
							default { '?' }
						}
					}
				}
				else { '' }
			}
		} | foreach {
			#finally write them all out
			" $($_.ObjectType)($($_.ObjectName))  $($_.change)"
		}
		
		#Now add in any completely deleted objects that wouldn't be in the current model
		if ($changes.count -gt 0)
		{
			$ObjectPaths = $model | Display-object -depth 3 | foreach{ "$($_.path)" }
			$changes |
			foreach{ $Bits = $_.ref.split('.'); "`$.$($bits[1] + '.' + $bits[2] + '.' + $bits[3])" } |
			select -unique | where { $_ -notin $ObjectPaths } | foreach {
				#finally write them all out
				$EntityCode += " $($bits[2])($($bits[1] + $bits[3]))  $deleted"
			}
		}
		
		#now print out the 'hard entities'.
		$EntityRelations = $model | Display-object -depth 10 |
		where{ $_.path -like '$.*.*.*.foreign key.*.Foreign Table' } |
		foreach{
			$bits = $_.Path.split('.');
			[pscustomobject]@{
				'TableSchema' = "$($Bits[1])";
				'TableName' = "$($bits[3])";
				'Key' = "$($bits[5])";
				'ReferenceSchema' = "$($_.Value)".Split('.')[0];
				'ReferenceTable' = "$($_.Value)".Split('.')[1]
			}
		} | foreach {
			"$($_.TableSchema).$($_.TableName)  }|..|| $($_.ReferenceSchema).$($_.ReferenceTable)   "
			
		}
<# now print out the plantuml header and the code. #>
		$pumlCode = @"
@startuml
skinparam class {
BackgroundColor WhiteSmoke
ArrowColor black
BorderColor gray
}
skinparam wrapWidth 150
skinparam handwritten true
skinparam monochrome false
skinparam packageStyle rect
skinparam defaultFontName Buxton Sketch
skinparam shadowing true
skinparam MessageAlign left
skinparam header{
  FontColor black
  FontSize 14
}
skinparam footer{
  FontColor black
  FontSize 10
}
left to right direction

Title $title
header Date: $(((get-date).Date).ToString().replace(' 00:00:00', '')
		)
footer <back:pink>Pink background</back> means deleted <back:lightgreen> green Background</back>means added and <back:gold>Gold background</back> means altered.

!define table(x) class x << (T,mistyrose) >>
!define user_table(x) class x << (T,mistyrose) >>
!define view(x) class x << (V,lightblue) >>
!define dml_trigger(x) class x << (R,red) >>
!define table_valued_function(x) class x << (F,darkorange) >>
!define aggregate_function(x) class x << (F,white) >>
!define scalar_function(x) class x << (F,plum) >>
!define clr_scalar_function(x) class x << (F,tan) >>
!define clr_table_valued_function(x) class x << (F,wheat) >>
!define inline_table_valued_function(x) class x << (F,gaisboro) >>
!define stored_procedure(x) class x << (P,indianred) >>
!define clr_stored_procedure(x) class x << (P,lemonshiffon) >>
!define extended_stored_procedure(x) class x << (P,linen) >>


$($EntityCode -join "`r`n") 
$($EntityRelations -join "`r`n  ")

@enduml
"@
		$pumlCode > "$MyPUMLFile"
	} #if parameters were OK
	if ($problems.Count -gt 0)
	{
		$Param1.Problems.'WriteOutERDiagramCode' += $problems;
	}
	else
	{
		$Param1.WriteLocations.'WriteOutERDiagramCode' = $MyPUMLFile;
	}
}



$SaveFlywaySchemaHistoryIfNecessary = {
	Param ($param1) # $SaveFlywaySchemaHistoryIfNecessary - dont delete this
	$problems = @() #none yet!
	$warnings = @()
	$feedback = @()
	$WriteLocations = @()
	$unnecessary = $false;
	
	$info = Flyway info -outputType=json # collect Flyway's info as a JSON file
	if ($info.error -ne $null) { $Problems += $info.error.message }
	else
	{
		$InfoObject = $info | convertFrom-json #convert it to something Powershell can read
		$SaveAsFile = "$($param1.Reportlocation)\current\Migrationinfo.json"
		#where we store the reports. We get this from the framework from $param1
		$Report = $InfoObject.psobject.Properties | where { $_.name -ne 'migrations' } |
		foreach{ @{ $_.name = $_.value } } #Get all the base info 
		#add to this, the extra info that we collect from the framework
		$Report += @('Branch', 'Variant', 'Server', 'Database', 'InstalledBy', 'previous') | foreach{
			@{ $_ = "$($param1.$_)" }
		}
		#save it to the report for the current migration
		$Report | convertTo-json > $SaveAsFile
		# now we get the migration collection that came from RFlkyway Info
		$infoObject.migrations | where {$_.state -eq 'success'}| foreach{
			if ($_.version -eq '') # it is the initial state before a migration is applied
			{
				# Write this to the base of the report location just once
				$SaveAsFile = "$($param1.Reportlocation)\Creationinfo.json"
				if (!(Test-Path $SaveAsFile -PathType leaf))
				{
					convertTo-json $_ > $SaveAsFile;
					$WriteLocations += $SaveAsFile;
				}
				else
				{ $feedback += "$SaveAsFile with the database creation info already existed" }
			}
			else #it must be a valid migration version
			{
				#write it to the current version report folder just once
                if (!(Test-Path "$($param1.Reportlocation)\$($_.Version)" -PathType Container))
                    {New-Item -ItemType directory -Path "$($param1.Reportlocation)\$($_.Version)"}
                if (!(Test-Path "$($param1.Reportlocation)\$($_.Version)\reports" -PathType Container))
                    {New-Item -ItemType directory -Path "$($param1.Reportlocation)\$($_.Version)\reports"}
				$SaveAsFile = "$($param1.Reportlocation)\$($_.Version)\reports\ApplyInfo.json"
				if (!(Test-Path $SaveAsFile -PathType leaf))
				{
					convertTo-json $_ > $SaveAsFile;
					$WriteLocations += $SaveAsFile;
				}
				else
				{ $feedback += "$SaveAsFile already existed" }
			}
		}
	}
	if ($WriteLocations.Count -gt 0)
	{
		$Param1.WriteLocations.'SaveFlywaySchemaHistoryIfNecessary' = $writeLocations
	};
	if ($feedback.Count -gt 0)
	{
		$Param1.Feedback.'SaveFlywaySchemaHistoryIfNecessary' = $feedback
	};
	if ($problems.Count -gt 0)
	{
		$Param1.Problems.'SaveFlywaySchemaHistoryIfNecessary' += $problems;
	}
	if ($warnings.Count -gt 0)
	{
		$Param1.warnings.'SaveFlywaySchemaHistoryIfNecessary' += $warnings;
	}
}


Function GetorSetPassword{
[CmdletBinding()]
	param
	(
		[Parameter(Mandatory = $true,
				   Position = 1)]
		[string]$uid,
		[Parameter(Mandatory = $true,
				   Position = 2)]
		[string]$Server,
		[Parameter(Mandatory = $false,
				   Position = 3)]
		[string]$RDBMS =$null) #change to your  database system if you have two on the one server!





    $PwdDetails= @{
        'RDBMS'=$RDBMS; #jdbc name. Only necessary for systems with several RDBMS on the same server
	    'Server'=$server;
        'pwd' = ''; #Always leave blank
	    'uid' = $uid; #leave blank unless you use credentials
	    'problems' = @{ }; # for reporting any big problems
         }
 
    $FetchAnyRequiredPasswords.Invoke($PwdDetails);
    if ($PwdDetails.Problems.FetchAnyRequiredPasswords.Count -gt 0)
         {Write-error "$($PwdDetails.Problems.FetchAnyRequiredPasswords)" }
    $PwdDetails.pwd
}


<#
	.SYNOPSIS
		executes a list of scriptblock tasks
	
	.PARAMETER DatabaseDetails
		a hashtable with the Flyway database details
	
	.PARAMETER Invocations
		the list of scriptblocks to execute
	
	.PARAMETER AddedParameters
		Any Additional parameters to the scriptblock(s) other than the hashtable
	

#>
function Process-FlywayTasks
{
	[CmdletBinding()]
	param
	(
		[Parameter(Mandatory = $true,
				   Position = 1)]
		[System.Collections.Hashtable]$DatabaseDetails,
		[Parameter(Mandatory = $true,
				   Position = 2)]
		[array]$Invocations,
		[Parameter(Position = 3)]
		[array]$AddedParameters = $null
	)
	$parameters = @()
	$parameters += $DatabaseDetails;
	if ($AddedParameters -ne $null) { $parameters += $AddedParameters }
	$Invocations | foreach{
		if ($_ -eq $null)
		{
			Write-error "the scriptblock wasn't recognised"
		}
		elseif ($DatabaseDetails.Problems.Count -eq 0)
		{
			#try to get the name of the task
			if ($_.Ast.Extent.Text -imatch '(?<=# ?\$)([\w]{5,80})')
			{ $TaskName = $matches[0] }
			else { $TaskName = 'unknown' }
			try # try executing the script block
			{ $returnedArray = $_.Invoke($parameters) }
			catch #if it hit an exception
			{
				# handle the exception
				$where = $PSItem.InvocationInfo.PositionMessage
				$ErrorMessage = $_.Exception.Message
				$FailedItem = $_.Exception.ItemName
				$DatabaseDetails.Problems.exceptions += "$Taskname failed with $FailedItem : $ErrorMessage at `n $where."
			}
			"Executed $TaskName"
		}
		
	}
	#print out any errors and warnings. 
	if ($DatabaseDetails.Problems.Count -gt 0) #list out every problem and which task failed
	{
		$DatabaseDetails.Problems.GetEnumerator() |
		Foreach{ Write-warning "Problem! $($_.Key)---------"; $_.Value } |
		foreach { write-warning "`t$_" }
		$DatabaseDetails.Problems = @{ }
		
	}
	if ($DatabaseDetails.Warnings.Count -gt 0) #list out exery warning and which task failed
	{
		$DatabaseDetails.Warnings.GetEnumerator() |
		Foreach{ Write-warning "$($_.Key)---------"; $_.Value } |
		foreach { write-warning  "`t$_" }
		$DatabaseDetails.Warnings = @{ }
	}
	
	$DatabaseDetails.WriteLocations.GetEnumerator() |
	Foreach{
		Write-Output "For the $($_.Key), we saved the report in $($_.Value)"
		$DatabaseDetails.WriteLocations = @{ }
	}
	
	$DatabaseDetails.feedback.GetEnumerator() |
	Foreach{
		Write-Output "in $($_.Key), $($_.Value)"
		$DatabaseDetails.feedback = @{ }
	}
	
}

<#
	.SYNOPSIS
		Executes SQL queries that return results according to the RDBMS you have
	
	.DESCRIPTION
		This is a simple front-end for all the scritblocks that can be used to execute SQL for its corresponding RDBMS. This isn't the quickest way of doing it because a connection isn't maintained but, hell, we are scripting.
	
	.PARAMETER DatabaseDetails
		the hashtable with the project parmeters .
	
	.PARAMETER query
		the sql query
	
	.PARAMETER fileBasedQuery
		a query. If a file, put the path in the $fileBasedQuery parameter
	
	.NOTES
		Additional information about the function.
#>
function Execute-SQL
{
	[CmdletBinding()]
	param
	(
		[Parameter(Mandatory = $true,
				   Position = 1)]
		[System.Collections.Hashtable]$DatabaseDetails,
		[Parameter(Mandatory = $true,
				   Position = 2)]
		[String]$query,
		[Parameter(Position = 3)]
		[String]$fileBasedQuery = $null
	)
	
	$Scriptblock = switch ($DatabaseDetails.RDBMS)
	{
		'postgresql'   { $GetdataFromPsql }
		'sqlserver'  { $GetdataFromSQLCMD }
		'sqlite' { $GetdataFromSqlite }
        'mariadb' { $GetdataFromMySQL }
        'Mysql' { $GetdataFromMySQL }
        	}
	$ErrorsSoFar = $Error.count
	$Scriptblock.invoke($DatabaseDetails,$query,$fileBasedQuery) ;
	if ($Error.Count -gt $ErrorsSoFar)
	{ 0 .. ($Error.Count - $ErrorsSoFar-1) | foreach{ Write-warning "$($error[$_])" } }
}

<#
	.SYNOPSIS
		Executes SQL queries that return results according to the RDBMS you have
	
	.DESCRIPTION
		This is a simple front-end for all the scritblocks that can be used to execute SQL for its corresponding RDBMS. This isn't the quickest way of doing it because a connection isn't maintained but, hell, we are scripting.
	
	.PARAMETER DatabaseDetails
		the hashtable with the project parmeters .
	
	.PARAMETER query
		the sql query
	
	.PARAMETER fileBasedQuery
		a query. If a file, put the path in the $fileBasedQuery parameter
	
	.NOTES
		Additional information about the function.
#>
function Execute-SQLStatement
{
	[CmdletBinding()]
	param
	(
		[Parameter(Mandatory = $true,
				   Position = 1)]
		[System.Collections.Hashtable]$DatabaseDetails,
		[Parameter(Mandatory = $true,
				   Position = 2)]
		[String]$Statement
	)
	
	$Scriptblock = switch ($DatabaseDetails.RDBMS)
	{
		'postgresql'   { $GetdataFromPsql }
		'sqlserver'  { $GetdataFromSQLCMD }
		'sqlite' { $GetdataFromSqlite }
        'mariadb' { $GetdataFromMySQL }
        'Mysql' { $GetdataFromMySQL }
        	}
	$ErrorsSoFar = $Error.count
	$Scriptblock.invoke($DatabaseDetails,$Statement,$null,$true) ;
	if ($Error.Count -gt $ErrorsSoFar)
	{ 0 .. ($Error.Count - $ErrorsSoFar-1) | foreach{ Write-warning "$($error[$_])" } }
}



'FlywayTeamwork framework  loaded. V1.2.134'


